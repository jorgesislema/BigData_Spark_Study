# ğŸ”„ Transformaciones BÃ¡sicas en Apache Spark

## ğŸ”¥ IntroducciÃ³n
Las **transformaciones** en Apache Spark son operaciones que se aplican sobre **RDDs (Resilient Distributed Datasets) o DataFrames** para generar nuevos conjuntos de datos sin modificar los originales. Son **perezosas** (*lazy evaluation*), lo que significa que no se ejecutan hasta que se invoca una **acciÃ³n**.

---

## ğŸ“Œ Tipos de Transformaciones en Spark
En Spark, las transformaciones pueden ser de dos tipos:
- **Narrow Transformations** (Transformaciones estrechas): Cada particiÃ³n del RDD hijo depende de una Ãºnica particiÃ³n del RDD padre. Ejemplo: `map()`, `filter()`.
- **Wide Transformations** (Transformaciones amplias): Requieren una reorganizaciÃ³n de datos entre nodos, generando una fase de *shuffle*. Ejemplo: `groupBy()`, `reduceByKey()`.

---

## ğŸ› ï¸ Transformaciones Comunes en PySpark
### ğŸ”¹ 1. `map()` â€“ Aplicar una funciÃ³n a cada elemento
Transforma cada elemento del RDD aplicando una funciÃ³n dada.
```python
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
result = rdd.map(lambda x: x * 2)
print(result.collect())  # [2, 4, 6, 8, 10]
```

### ğŸ”¹ 2. `filter()` â€“ Filtrar elementos de un RDD
Devuelve solo los elementos que cumplen una condiciÃ³n.
```python
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
result = rdd.filter(lambda x: x % 2 == 0)
print(result.collect())  # [2, 4]
```

### ğŸ”¹ 3. `flatMap()` â€“ TransformaciÃ³n con divisiÃ³n de elementos
Similar a `map()`, pero cada elemento puede producir mÃºltiples valores.
```python
rdd = spark.sparkContext.parallelize(["Hola mundo", "Apache Spark"])
result = rdd.flatMap(lambda x: x.split(" "))
print(result.collect())  # ['Hola', 'mundo', 'Apache', 'Spark']
```

### ğŸ”¹ 4. `distinct()` â€“ Eliminar duplicados
Elimina elementos repetidos dentro del RDD.
```python
rdd = spark.sparkContext.parallelize([1, 2, 2, 3, 3, 3])
result = rdd.distinct()
print(result.collect())  # [1, 2, 3]
```

### ğŸ”¹ 5. `groupByKey()` â€“ Agrupar por clave
Agrupa elementos con la misma clave, sin reducirlos.
```python
rdd = spark.sparkContext.parallelize([("a", 1), ("b", 2), ("a", 3)])
result = rdd.groupByKey()
print([(x, list(y)) for x, y in result.collect()])  # [('a', [1, 3]), ('b', [2])]
```

### ğŸ”¹ 6. `reduceByKey()` â€“ Reducir valores por clave
Similar a `groupByKey()`, pero aplica una funciÃ³n de reducciÃ³n.
```python
rdd = spark.sparkContext.parallelize([("a", 1), ("b", 2), ("a", 3)])
result = rdd.reduceByKey(lambda x, y: x + y)
print(result.collect())  # [('a', 4), ('b', 2)]
```

---

## ğŸï¸ ComparaciÃ³n entre `groupByKey()` y `reduceByKey()`
| TransformaciÃ³n | Uso | Desventaja |
|--------------|----|------------|
| `groupByKey()` | Agrupa valores por clave en una lista | Puede consumir mÃ¡s memoria |
| `reduceByKey()` | Aplica reducciÃ³n sin crear listas intermedias | Solo para operaciones reducibles |

---

## ğŸ¯ ConclusiÃ³n
Las transformaciones en Spark permiten manipular grandes volÃºmenes de datos de forma eficiente. Comprender cuÃ¡ndo usar **`map()`**, **`filter()`**, **`groupByKey()`** y otras transformaciones bÃ¡sicas es clave para optimizar el rendimiento de procesamiento en **Big Data**. ğŸš€

