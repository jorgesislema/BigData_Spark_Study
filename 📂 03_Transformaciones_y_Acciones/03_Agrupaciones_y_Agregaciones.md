# ğŸ“Š Agrupaciones y Agregaciones en Apache Spark

## ğŸ”¥ IntroducciÃ³n
En **Apache Spark**, las operaciones de **agrupaciÃ³n y agregaciÃ³n** permiten resumir y analizar grandes volÃºmenes de datos de manera eficiente. Estas operaciones son ampliamente utilizadas en tareas como generaciÃ³n de reportes, anÃ¡lisis de tendencias y procesamiento de datos a gran escala.

---

## ğŸ“Œ Diferencia entre Agrupaciones y Agregaciones
| OperaciÃ³n | DescripciÃ³n |
|-----------|------------|
| **AgrupaciÃ³n (`groupBy`)** | Agrupa los datos en funciÃ³n de un campo especÃ­fico, generando un conjunto de grupos. |
| **AgregaciÃ³n (`agg`)** | Aplica funciones de agregaciÃ³n (como suma, promedio, conteo) sobre los grupos de datos creados. |

---

## ğŸ› ï¸ Agrupaciones en PySpark

### ğŸ”¹ `groupBy()` â€“ Agrupar Datos
La funciÃ³n `groupBy()` permite agrupar un DataFrame segÃºn una o mÃ¡s columnas.
```python
from pyspark.sql import SparkSession
from pyspark.sql.functions import count

spark = SparkSession.builder.appName("Agrupaciones").getOrCreate()

# Crear un DataFrame de ejemplo
data = [("Alice", "Ventas", 5000), ("Bob", "Ventas", 6000),
        ("Charlie", "IT", 7000), ("David", "IT", 8000)]
df = spark.createDataFrame(data, ["Nombre", "Departamento", "Salario"])

# Agrupar por Departamento
df_grouped = df.groupBy("Departamento").count()
df_grouped.show()
```
Salida esperada:
```
+------------+-----+
|Departamento|count|
+------------+-----+
|      Ventas|    2|
|         IT|    2|
+------------+-----+
```

---

## ğŸ“‚ Agregaciones en PySpark

### ğŸ”¹ `agg()` â€“ Aplicar Funciones de AgregaciÃ³n
Podemos utilizar `agg()` para realizar mÃºltiples agregaciones en los datos agrupados.
```python
from pyspark.sql.functions import sum, avg, max, min

# Aplicar funciones de agregaciÃ³n
df_agg = df.groupBy("Departamento").agg(
    sum("Salario").alias("Total_Salarios"),
    avg("Salario").alias("Salario_Promedio"),
    max("Salario").alias("Salario_Max"),
    min("Salario").alias("Salario_Min")
)
df_agg.show()
```
Salida esperada:
```
+------------+--------------+---------------+------------+------------+
|Departamento|Total_Salarios|Salario_Promedio|Salario_Max|Salario_Min|
+------------+--------------+---------------+------------+------------+
|      Ventas|        11000 |         5500.0|        6000|        5000|
|         IT |        15000 |         7500.0|        8000|        7000|
+------------+--------------+---------------+------------+------------+
```

---

## âš¡ Funciones de AgregaciÃ³n en Spark SQL
TambiÃ©n podemos usar **Spark SQL** para realizar agregaciones con consultas SQL.
```python
# Registrar el DataFrame como tabla temporal
df.createOrReplaceTempView("empleados")

# Ejecutar una consulta SQL con agregaciones
resultado = spark.sql("""
    SELECT Departamento, SUM(Salario) AS Total_Salarios,
           AVG(Salario) AS Salario_Promedio
    FROM empleados
    GROUP BY Departamento
""")
resultado.show()
```

---

## ğŸï¸ ComparaciÃ³n entre `groupBy()` y `reduceByKey()`
| OperaciÃ³n | Uso | Ventaja |
|-----------|-----|---------|
| `groupBy()` | Se usa en DataFrames | Permite aplicar mÃºltiples funciones de agregaciÃ³n |
| `reduceByKey()` | Se usa en RDDs | Optimizado para agregaciones en grandes volÃºmenes de datos |

---

## ğŸ¯ ConclusiÃ³n
Las **agrupaciones y agregaciones en Spark** son esenciales para el procesamiento eficiente de datos. Mientras que `groupBy()` es ideal para trabajar con DataFrames y aplicar mÃºltiples funciones de agregaciÃ³n, **`reduceByKey()`** sigue siendo una excelente opciÃ³n en **RDDs** para cÃ¡lculos optimizados. ğŸš€

