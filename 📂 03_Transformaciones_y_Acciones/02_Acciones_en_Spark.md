# âš¡ Acciones en Apache Spark

## ğŸ”¥ IntroducciÃ³n
En **Apache Spark**, las **acciones** son operaciones que devuelven un resultado al controlador (driver) despuÃ©s de aplicar transformaciones a un **RDD** o **DataFrame**. A diferencia de las transformaciones (*lazy evaluation*), las acciones **desencadenan la ejecuciÃ³n real del cÃ¡lculo en el clÃºster**.

---

## ğŸ“Œ CaracterÃ­sticas de las Acciones en Spark
âœ… **Ejecutan transformaciones previas** y generan un resultado.
âœ… **Devuelven valores al driver o guardan datos en almacenamiento**.
âœ… **Pueden generar cÃ¡lculos costosos** si se llaman varias veces.

---

## ğŸ› ï¸ Acciones Comunes en PySpark

### ğŸ”¹ 1. `collect()` â€“ Recuperar todos los elementos
Devuelve todos los elementos de un **RDD** o **DataFrame** como una lista en el driver.
```python
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
result = rdd.collect()
print(result)  # [1, 2, 3, 4, 5]
```
âš ï¸ **Cuidado**: No usar con grandes volÃºmenes de datos ya que puede agotar la memoria del driver.

### ğŸ”¹ 2. `count()` â€“ Contar elementos
Cuenta la cantidad total de elementos en un **RDD** o **DataFrame**.
```python
rdd = spark.sparkContext.parallelize([1, 2, 3, 4, 5])
print(rdd.count())  # 5
```

### ğŸ”¹ 3. `first()` â€“ Obtener el primer elemento
Devuelve el primer elemento del conjunto de datos.
```python
rdd = spark.sparkContext.parallelize([10, 20, 30])
print(rdd.first())  # 10
```

### ğŸ”¹ 4. `take(n)` â€“ Obtener los primeros *n* elementos
Devuelve una lista con los primeros *n* elementos.
```python
rdd = spark.sparkContext.parallelize([10, 20, 30, 40, 50])
print(rdd.take(3))  # [10, 20, 30]
```

### ğŸ”¹ 5. `reduce()` â€“ Reducir elementos a un solo valor
Aplica una funciÃ³n de agregaciÃ³n para reducir los datos.
```python
rdd = spark.sparkContext.parallelize([1, 2, 3, 4])
result = rdd.reduce(lambda a, b: a + b)
print(result)  # 10 (1+2+3+4)
```

### ğŸ”¹ 6. `foreach()` â€“ Aplicar una funciÃ³n a cada elemento
Ejecuta una funciÃ³n en cada elemento, Ãºtil para escribir en bases de datos o almacenamiento.
```python
def imprimir_elemento(x):
    print(x)

rdd = spark.sparkContext.parallelize(["A", "B", "C"])
rdd.foreach(imprimir_elemento)
```

### ğŸ”¹ 7. `saveAsTextFile()` â€“ Guardar datos en archivos
Guarda el resultado en un archivo de texto dentro de un directorio.
```python
rdd = spark.sparkContext.parallelize(["Hola", "Mundo", "Spark"])
rdd.saveAsTextFile("/ruta/salida")
```

---

## ğŸï¸ ComparaciÃ³n entre `collect()` y `take()`
| AcciÃ³n | Uso | Desventaja |
|--------|-----|------------|
| `collect()` | Devuelve todos los datos al driver | Puede agotar la memoria en grandes datasets |
| `take(n)` | Devuelve solo los primeros *n* elementos | Menos costoso que `collect()` |

---

## ğŸ¯ ConclusiÃ³n
Las **acciones en Spark** permiten obtener resultados y desencadenar la ejecuciÃ³n de transformaciones. Es importante elegir la acciÃ³n adecuada para evitar sobrecarga en el **driver** y mejorar el rendimiento en entornos distribuidos. ğŸš€

