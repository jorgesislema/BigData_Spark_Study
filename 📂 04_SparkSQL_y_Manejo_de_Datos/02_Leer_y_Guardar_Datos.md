# ğŸ“‚ Lectura y Escritura de Datos en Apache Spark

## ğŸ”¥ IntroducciÃ³n
Apache Spark permite leer y escribir datos en mÃºltiples formatos como **CSV, JSON, Parquet, ORC, Avro y JDBC**. Esto facilita la integraciÃ³n con bases de datos, sistemas de almacenamiento y anÃ¡lisis de Big Data.

---

## ğŸ“Œ Leer Datos en Spark
### ğŸ”¹ Leer un archivo CSV
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("LeerCSV").getOrCreate()
df = spark.read.format("csv").option("header", "true").option("inferSchema", "true").load("/ruta/datos.csv")
df.show()
```
ğŸ“Œ **ExplicaciÃ³n:**
- `format("csv")`: Especifica el formato del archivo.
- `option("header", "true")`: Usa la primera fila como nombres de columnas.
- `option("inferSchema", "true")`: Detecta automÃ¡ticamente los tipos de datos.

### ğŸ”¹ Leer un archivo JSON
```python
df_json = spark.read.json("/ruta/datos.json")
df_json.show()
```

### ğŸ”¹ Leer un archivo Parquet
```python
df_parquet = spark.read.parquet("/ruta/datos.parquet")
df_parquet.show()
```

### ğŸ”¹ Leer desde una Base de Datos JDBC
```python
df_jdbc = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:mysql://localhost:3306/mi_bd") \
    .option("dbtable", "usuarios") \
    .option("user", "root") \
    .option("password", "password") \
    .load()
df_jdbc.show()
```

---

## ğŸ“ Guardar Datos en Spark
### ğŸ”¹ Guardar como CSV
```python
df.write.format("csv").option("header", "true").save("/ruta/output.csv")
```

### ğŸ”¹ Guardar como JSON
```python
df.write.json("/ruta/output.json")
```

### ğŸ”¹ Guardar como Parquet
```python
df.write.parquet("/ruta/output.parquet")
```

### ğŸ”¹ Guardar en una Base de Datos JDBC
```python
df.write \
    .format("jdbc") \
    .option("url", "jdbc:mysql://localhost:3306/mi_bd") \
    .option("dbtable", "usuarios_guardados") \
    .option("user", "root") \
    .option("password", "password") \
    .save()
```

---

## ğŸï¸ Consideraciones de Rendimiento
âœ… **Usar Parquet en lugar de CSV** para mejor compresiÃ³n y velocidad.
âœ… **Especificar particiones (`partitionBy`)** para grandes volÃºmenes de datos.
âœ… **Configurar `mode`** (`overwrite`, `append`, `ignore`, `error`) al escribir datos.

Ejemplo de particionado y modo de escritura:
```python
df.write.mode("overwrite").partitionBy("categoria").parquet("/ruta/output")
```

---

## ğŸ¯ ConclusiÃ³n
Leer y guardar datos en Spark es un proceso eficiente gracias a su compatibilidad con mÃºltiples formatos y su capacidad de optimizaciÃ³n. Usar formatos como **Parquet** y configurar correctamente las opciones mejora el rendimiento y la escalabilidad. ğŸš€

