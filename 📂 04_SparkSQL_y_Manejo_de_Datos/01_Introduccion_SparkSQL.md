# ğŸ—„ï¸ IntroducciÃ³n a Spark SQL

## ğŸ”¥ Â¿QuÃ© es Spark SQL?
**Spark SQL** es un mÃ³dulo de **Apache Spark** que permite procesar datos estructurados utilizando consultas **SQL**, APIs de DataFrames y **Dataset API**. Es una de las herramientas mÃ¡s potentes de Spark para manipular grandes volÃºmenes de datos con facilidad y optimizaciÃ³n.

---

## ğŸ“Œ Beneficios de Spark SQL
- âœ… **OptimizaciÃ³n automÃ¡tica** con **Catalyst Optimizer**.
- âœ… **Interoperabilidad** con SQL estÃ¡ndar y DataFrames.
- âœ… **Soporte para mÃºltiples fuentes de datos**: JSON, Parquet, Avro, JDBC, HDFS.
- âœ… **IntegraciÃ³n con herramientas BI** como Tableau y Power BI.

---

## ğŸ› ï¸ CreaciÃ³n de una SparkSession
Para usar **Spark SQL**, primero es necesario crear una **SparkSession**:
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("EjemploSparkSQL").getOrCreate()
```

---

## ğŸ“Š CreaciÃ³n de un DataFrame desde Datos
Podemos crear un **DataFrame** en Spark SQL a partir de una lista de datos:
```python
data = [("Alice", 25), ("Bob", 30), ("Charlie", 35)]
df = spark.createDataFrame(data, ["Nombre", "Edad"])
df.show()
```
Salida esperada:
```
+-------+----+
| Nombre|Edad|
+-------+----+
|  Alice|  25|
|    Bob|  30|
|Charlie|  35|
+-------+----+
```

---

## ğŸ›ï¸ Uso de Consultas SQL en Spark SQL
### ğŸ”¹ Registrar un DataFrame como Tabla Temporal
Para ejecutar consultas SQL sobre un **DataFrame**, primero debemos registrarlo como una tabla temporal:
```python
df.createOrReplaceTempView("personas")
```

### ğŸ”¹ Ejecutar Consultas SQL
Una vez registrado, podemos ejecutar consultas SQL directamente:
```python
resultado = spark.sql("SELECT * FROM personas WHERE Edad > 25")
resultado.show()
```
Salida esperada:
```
+-------+----+
| Nombre|Edad|
+-------+----+
|    Bob|  30|
|Charlie|  35|
+-------+----+
```

---

## ğŸ“‚ Lectura y Escritura de Archivos con Spark SQL
### ğŸ”¹ Leer un archivo JSON
```python
df_json = spark.read.json("/ruta/datos.json")
df_json.show()
```

### ğŸ”¹ Guardar un DataFrame en formato Parquet
```python
df.write.parquet("/ruta/output.parquet")
```

---

## âš¡ ComparaciÃ³n entre DataFrames y SQL en Spark
| CaracterÃ­stica  | DataFrame API  | Spark SQL  |
|---------------|---------------|------------|
| **Sintaxis** | Python/Scala API | Consultas SQL |
| **OptimizaciÃ³n** | SÃ­ (Catalyst) | SÃ­ (Catalyst) |
| **Facilidad de Uso** | Mejor para programaciÃ³n | Familiar para SQL users |
| **Compatibilidad** | Soporta Python, Scala, Java | Solo SQL |

---

## ğŸ¯ ConclusiÃ³n
Spark SQL combina el poder de **Apache Spark** con la facilidad de **SQL**, permitiendo trabajar con grandes volÃºmenes de datos de manera eficiente. Su optimizaciÃ³n automÃ¡tica lo hace una excelente opciÃ³n para anÃ¡lisis de datos a gran escala. ğŸš€

