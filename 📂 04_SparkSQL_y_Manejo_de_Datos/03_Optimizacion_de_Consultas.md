# âš¡ OptimizaciÃ³n de Consultas en Apache Spark

## ğŸ”¥ IntroducciÃ³n
Optimizar consultas en **Apache Spark** es clave para mejorar el rendimiento en el procesamiento de datos a gran escala. Spark utiliza el **Catalyst Optimizer** y el **Tungsten Engine** para optimizar consultas de manera automÃ¡tica, pero tambiÃ©n existen buenas prÃ¡cticas que pueden mejorar aÃºn mÃ¡s la eficiencia.

---

## ğŸ“Œ Estrategias para OptimizaciÃ³n de Consultas

### ğŸ”¹ 1. Usar Formatos de Archivo Optimizados
Los formatos **Parquet** y **ORC** ofrecen mayor compresiÃ³n y velocidad en comparaciÃ³n con CSV o JSON.
```python
df.write.mode("overwrite").parquet("/ruta/output.parquet")
```
âœ… **Beneficio:** Reduce el uso de almacenamiento y mejora la lectura.

### ğŸ”¹ 2. Cachear DataFrames Reutilizados
Si un **DataFrame** se usa varias veces en el cÃ³digo, cachearlo puede mejorar el rendimiento.
```python
df.cache()
df.count()  # Fuerza la cache para evitar recalculaciones
```
âœ… **Beneficio:** Evita recomputaciones innecesarias.

### ğŸ”¹ 3. Reducir el NÃºmero de Particiones
Por defecto, Spark crea muchas particiones. Podemos reducirlas para optimizar consultas.
```python
df = df.repartition(10)  # Redistribuye en 10 particiones
```
âœ… **Beneficio:** Menos sobrecarga en la administraciÃ³n de tareas.

### ğŸ”¹ 4. Usar `select()` en lugar de `*`
Seleccionar solo las columnas necesarias mejora la eficiencia.
```python
df.select("columna1", "columna2").show()
```
âœ… **Beneficio:** Disminuye el procesamiento de datos innecesarios.

### ğŸ”¹ 5. Filtrar Datos antes de Realizar Transformaciones
Aplicar filtros lo antes posible en el pipeline evita procesar datos innecesarios.
```python
df = df.filter(df["columna"] > 100)
```
âœ… **Beneficio:** Reduce la cantidad de datos procesados en transformaciones posteriores.

### ğŸ”¹ 6. Uso de Broadcast Join
Para **Joins** con tablas pequeÃ±as, `broadcast()` puede mejorar el rendimiento.
```python
from pyspark.sql.functions import broadcast

df_resultado = df1.join(broadcast(df2), "columna_comun", "inner")
```
âœ… **Beneficio:** Reduce el trÃ¡fico de datos en la red y acelera los *joins*.

### ğŸ”¹ 7. Configurar `spark.sql.shuffle.partitions`
El nÃºmero de particiones en operaciones como `groupBy()` y `join()` puede ajustarse para mejorar el rendimiento.
```python
spark.conf.set("spark.sql.shuffle.partitions", 200)
```
âœ… **Beneficio:** Evita la sobrecarga de **shuffle** en operaciones de agregaciÃ³n.

---

## ğŸï¸ ComparaciÃ³n de Estrategias de OptimizaciÃ³n
| TÃ©cnica | Beneficio |
|---------|----------|
| **Usar Parquet/ORC** | Reduce tamaÃ±o y mejora lectura |
| **Cachear DataFrames** | Evita cÃ¡lculos repetidos |
| **Reparticionar datos** | Mejora paralelizaciÃ³n |
| **Select en lugar de `*`** | Reduce el procesamiento innecesario |
| **Filtrar antes de transformar** | Evita carga innecesaria |
| **Broadcast Join** | Acelera uniones con tablas pequeÃ±as |
| **Configurar `shuffle.partitions`** | Mejora rendimiento en joins y agrupaciones |

---

## ğŸ¯ ConclusiÃ³n
Aplicar estas estrategias de **optimizaciÃ³n en Spark** puede mejorar significativamente la eficiencia y reducir costos computacionales. ğŸš€

