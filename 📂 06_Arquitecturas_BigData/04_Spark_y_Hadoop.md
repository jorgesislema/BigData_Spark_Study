# ğŸ—ï¸ IntegraciÃ³n de Apache Spark con Hadoop

## ğŸ”¥ IntroducciÃ³n
Apache **Spark** y **Hadoop** se complementan para procesar grandes volÃºmenes de datos de manera eficiente. **Hadoop HDFS** proporciona almacenamiento distribuido, mientras que **Spark** permite ejecutar cÃ¡lculos en memoria, logrando una ejecuciÃ³n mÃ¡s rÃ¡pida que MapReduce.

Usar **Spark + Hadoop** es ideal para:
- Procesamiento distribuido de grandes datasets.
- IntegraciÃ³n con clÃºsteres Hadoop existentes.
- EjecuciÃ³n de ETL y Machine Learning a gran escala.

---

## ğŸ“Œ Requisitos Previos
Antes de conectar Spark con Hadoop, asegÃºrate de tener:
âœ… Un clÃºster **Hadoop HDFS** configurado y en ejecuciÃ³n.
âœ… Apache **Spark** instalado y configurado para acceder a HDFS.
âœ… Variables de entorno definidas (`HADOOP_HOME`, `SPARK_HOME`).

---

## ğŸ› ï¸ ConfiguraciÃ³n de Spark para Hadoop
### ğŸ”¹ Configurar Variables de Entorno
En Linux/Mac, edita `~/.bashrc`:
```bash
export HADOOP_HOME=/usr/local/hadoop
export SPARK_HOME=/usr/local/spark
export PATH=$PATH:$HADOOP_HOME/bin:$SPARK_HOME/bin
```

### ğŸ”¹ Agregar Dependencias
Si usas **PySpark**, inicia Spark con las librerÃ­as de Hadoop:
```bash
pyspark --packages org.apache.hadoop:hadoop-client:3.2.0
```
Para proyectos en **Scala/Java**, agrega en `build.sbt`:
```sbt
libraryDependencies += "org.apache.hadoop" % "hadoop-client" % "3.2.0"
```

---

## ğŸ”„ Leer Datos desde HDFS en Spark
### ğŸ”¹ Leer un archivo Parquet desde HDFS
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("SparkHadoop") \
    .config("spark.hadoop.fs.defaultFS", "hdfs://localhost:9000") \
    .getOrCreate()

df = spark.read.parquet("hdfs://localhost:9000/datos/output.parquet")
df.show()
```
âœ… **ExplicaciÃ³n:**
- `fs.defaultFS`: Define la direcciÃ³n de HDFS.
- `hdfs://localhost:9000`: Indica la ubicaciÃ³n del NameNode.

### ğŸ”¹ Leer un archivo CSV desde HDFS
```python
df = spark.read.option("header", "true").csv("hdfs://localhost:9000/datos/dataset.csv")
df.show()
```

---

## ğŸ“ Escritura de Datos en HDFS desde Spark
```python
df.write.mode("overwrite").parquet("hdfs://localhost:9000/datos/output.parquet")
```
âœ… **Modo `overwrite`**: Reemplaza los archivos existentes.

---

## âš¡ OptimizaciÃ³n y Buenas PrÃ¡cticas
âœ… **Usar formato Parquet en lugar de CSV** para mejorar el rendimiento.
âœ… **Evitar `collect()`** en grandes volÃºmenes de datos para prevenir sobrecarga en el driver.
âœ… **Configurar `dfs.replication`** para mejorar la tolerancia a fallos:
```bash
hdfs dfs -setrep -w 3 /datos/output.parquet
```

---

## ğŸ¯ ConclusiÃ³n
La integraciÃ³n de **Apache Spark y Hadoop** permite almacenar y procesar datos de manera distribuida con alto rendimiento. Configurar correctamente las conexiones y optimizar el acceso a los datos mejora la escalabilidad y eficiencia del sistema. ğŸš€

