# â˜ï¸ IntegraciÃ³n de Apache Spark con AWS S3

## ğŸ”¥ IntroducciÃ³n
Apache **Spark** y **AWS S3** permiten almacenar y procesar grandes volÃºmenes de datos de manera escalable y eficiente. **S3** es un almacenamiento distribuido en la nube, mientras que **Spark** ofrece procesamiento paralelo para grandes datasets.

La integraciÃ³n de **Spark + S3** es ideal para:
- Almacenar grandes volÃºmenes de datos en la nube.
- Ejecutar **ETL** sobre datos en S3.
- Procesar datos sin necesidad de clÃºsteres Hadoop tradicionales.

---

## ğŸ“Œ Requisitos Previos
Antes de conectar Spark con S3, asegÃºrate de tener:
âœ… Una cuenta de AWS con un **bucket S3** creado.
âœ… Apache Spark instalado con **Hadoop AWS**.
âœ… Credenciales de AWS configuradas.

---

## ğŸ› ï¸ ConfiguraciÃ³n de Spark para AWS S3
### ğŸ”¹ Agregar Dependencias
Si usas **PySpark**, inicia Spark con las librerÃ­as de AWS:
```bash
pyspark --packages org.apache.hadoop:hadoop-aws:3.2.0
```
Para proyectos en **Scala/Java**, agrega en `build.sbt`:
```sbt
libraryDependencies += "org.apache.hadoop" % "hadoop-aws" % "3.2.0"
```

### ğŸ”¹ Configurar Credenciales de AWS en Spark
```python
spark = SparkSession.builder \
    .appName("SparkAWS") \
    .config("spark.hadoop.fs.s3a.access.key", "TU_ACCESS_KEY") \
    .config("spark.hadoop.fs.s3a.secret.key", "TU_SECRET_KEY") \
    .config("spark.hadoop.fs.s3a.endpoint", "s3.amazonaws.com") \
    .getOrCreate()
```
âœ… **Alternativa**: Configurar credenciales en `~/.aws/credentials`.

---

## ğŸ”„ Leer Datos desde AWS S3 en Spark
### ğŸ”¹ Leer un archivo Parquet desde S3
```python
df = spark.read.parquet("s3a://mi-bucket/datos.parquet")
df.show()
```

### ğŸ”¹ Leer un archivo CSV desde S3
```python
df = spark.read.option("header", "true").csv("s3a://mi-bucket/datos.csv")
df.show()
```

---

## ğŸ“ Escritura de Datos en AWS S3 desde Spark
```python
df.write.mode("overwrite").parquet("s3a://mi-bucket/output.parquet")
```
âœ… **Modo `overwrite`**: Reemplaza el archivo si ya existe.

---

## âš¡ OptimizaciÃ³n y Buenas PrÃ¡cticas
âœ… **Usar formato Parquet o Avro** para mejorar la compresiÃ³n y velocidad.
âœ… **Evitar `collect()`** en grandes volÃºmenes de datos para prevenir sobrecarga.
âœ… **Configurar `fs.s3a.multipart.size`** para mejorar rendimiento en escrituras grandes.
```python
spark.conf.set("spark.hadoop.fs.s3a.multipart.size", "104857600")  # 100 MB
```

---

## ğŸ¯ ConclusiÃ³n
La integraciÃ³n de **Apache Spark y AWS S3** permite el procesamiento eficiente de datos en la nube sin necesidad de clÃºsteres tradicionales. Configurar correctamente la conexiÃ³n y optimizar el acceso a los datos mejora la eficiencia del sistema. ğŸš€

