# âš¡ IntegraciÃ³n de Apache Spark con Apache Cassandra

## ğŸ”¥ IntroducciÃ³n
Apache **Spark** y Apache **Cassandra** forman una poderosa combinaciÃ³n para el procesamiento de datos distribuidos. Mientras que **Cassandra** es una base de datos NoSQL altamente escalable y tolerante a fallos, **Spark** permite realizar anÃ¡lisis de datos en memoria de forma eficiente.

Usar **Spark + Cassandra** es ideal para:
- Procesamiento de datos en tiempo real.
- Consultas analÃ­ticas sobre grandes volÃºmenes de datos.
- IntegraciÃ³n con pipelines de Big Data.

---

## ğŸ“Œ Requisitos Previos
Antes de conectar Spark con Cassandra, asegÃºrate de tener:
âœ… Apache Cassandra instalado y ejecutÃ¡ndose.
âœ… Apache Spark con soporte para Spark SQL.
âœ… LibrerÃ­a `spark-cassandra-connector`.

---

## ğŸ› ï¸ ConfiguraciÃ³n de Spark para Cassandra
### ğŸ”¹ Agregar Dependencias
Para conectar Spark con Cassandra, usa el conector oficial.

Si usas **PySpark**, agrega la librerÃ­a al iniciar Spark:
```bash
pyspark --packages com.datastax.spark:spark-cassandra-connector_2.12:3.2.0
```
Para proyectos en **Scala/Java**, agrega en `build.sbt`:
```sbt
libraryDependencies += "com.datastax.spark" %% "spark-cassandra-connector" % "3.2.0"
```

---

## ğŸ”„ Leer Datos desde Cassandra en Spark
### ğŸ”¹ Conectarse a una Tabla de Cassandra
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("SparkCassandraIntegration") \
    .config("spark.cassandra.connection.host", "localhost") \
    .getOrCreate()

df = spark.read \
    .format("org.apache.spark.sql.cassandra") \
    .options(table="usuarios", keyspace="mi_keyspace") \
    .load()
df.show()
```
âœ… **ExplicaciÃ³n:**
- `spark.cassandra.connection.host`: DirecciÃ³n del nodo de Cassandra.
- `table`: Nombre de la tabla en Cassandra.
- `keyspace`: Keyspace de la base de datos.

---

## ğŸ“ Escritura de Datos en Cassandra desde Spark
```python
df.write \
    .format("org.apache.spark.sql.cassandra") \
    .options(table="usuarios", keyspace="mi_keyspace") \
    .mode("append") \
    .save()
```
âœ… **Modo `append`**: Inserta nuevos registros sin sobrescribir los existentes.

---

## âš¡ OptimizaciÃ³n y Buenas PrÃ¡cticas
âœ… **Configurar particionamiento** para mejorar la distribuciÃ³n de datos.
```python
spark.conf.set("spark.sql.shuffle.partitions", "200")
```
âœ… **Usar `pushdown` para optimizar consultas** en Cassandra.
```python
spark.conf.set("spark.cassandra.input.consistency.level", "LOCAL_ONE")
```
âœ… **Evitar `collect()`** en grandes volÃºmenes de datos para prevenir problemas de memoria.
```python
df.limit(10).show()
```

---

## ğŸ¯ ConclusiÃ³n
La integraciÃ³n de **Apache Spark y Cassandra** permite realizar anÃ¡lisis en tiempo real y procesar datos de forma distribuida. Configurar correctamente la conexiÃ³n y optimizar consultas ayuda a mejorar el rendimiento y escalabilidad. ğŸš€