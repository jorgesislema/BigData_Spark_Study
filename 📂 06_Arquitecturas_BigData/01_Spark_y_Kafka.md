# âš¡ IntegraciÃ³n de Apache Spark con Apache Kafka

## ğŸ”¥ IntroducciÃ³n
Apache **Spark** y Apache **Kafka** son dos herramientas clave en el ecosistema de Big Data. **Kafka** es una plataforma de mensajerÃ­a en tiempo real, mientras que **Spark Streaming** permite procesar flujos de datos en tiempo real con baja latencia.

La combinaciÃ³n de **Spark + Kafka** es ideal para casos de uso como:
- Procesamiento de datos en tiempo real.
- AnÃ¡lisis de logs y mÃ©tricas.
- ETL en streaming.
- Sistemas de detecciÃ³n de fraude y monitoreo.

---

## ğŸ“Œ Requisitos Previos
Antes de integrar Spark con Kafka, asegÃºrate de tener:
âœ… Apache Kafka en ejecuciÃ³n.
âœ… Apache Spark con soporte para **Spark Structured Streaming**.
âœ… LibrerÃ­as de integraciÃ³n (`spark-sql-kafka-0-10`).

---

## ğŸ› ï¸ ConfiguraciÃ³n de Spark para Kafka
### ğŸ”¹ Agregar Dependencias
Si usas **PySpark**, agrega la librerÃ­a de Kafka al ejecutar Spark:
```bash
pyspark --packages org.apache.spark:spark-sql-kafka-0-10_2.12:3.2.1
```
Para proyectos en **Scala/Java**, agrega en `build.sbt`:
```sbt
libraryDependencies += "org.apache.spark" %% "spark-sql-kafka-0-10" % "3.2.1"
```

---

## ğŸ”„ Consumiendo Datos desde Kafka en Spark
### ğŸ”¹ Conectarse a un Topic de Kafka
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("SparkKafkaIntegration") \
    .getOrCreate()

# Leer mensajes desde Kafka
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "mi_topic") \
    .option("startingOffsets", "earliest") \
    .load()

df.selectExpr("CAST(key AS STRING)", "CAST(value AS STRING)").show()
```
âœ… **ExplicaciÃ³n:**
- `kafka.bootstrap.servers`: Servidor de Kafka.
- `subscribe`: Nombre del topic.
- `startingOffsets`: Define desde dÃ³nde leer los mensajes (`earliest`, `latest`).

---

## ğŸ“ Escritura de Datos desde Spark a Kafka
### ğŸ”¹ Enviar Datos Procesados a Kafka
```python
query = df.selectExpr("CAST(value AS STRING) AS value") \
    .writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("topic", "output_topic") \
    .option("checkpointLocation", "/tmp/kafka-checkpoint") \
    .start()
```
âœ… **Importante:** `checkpointLocation` es obligatorio para mantener el estado del streaming.

---

## âš¡ OptimizaciÃ³n y Buenas PrÃ¡cticas
âœ… **Usar mÃºltiples particiones en Kafka** para paralelizar el procesamiento.
âœ… **Configurar `maxOffsetsPerTrigger`** para controlar la carga de datos.
```python
.option("maxOffsetsPerTrigger", "1000")
```
âœ… **Utilizar `watermark` para manejar datos retrasados** en eventos de streaming.
```python
df.withWatermark("timestamp", "10 minutes")
```

---

## ğŸ¯ ConclusiÃ³n
Apache Spark y Kafka forman una combinaciÃ³n poderosa para el procesamiento de datos en tiempo real. Configurar correctamente las conexiones, offsets y particiones permite construir aplicaciones de streaming escalables y eficientes. ğŸš€

