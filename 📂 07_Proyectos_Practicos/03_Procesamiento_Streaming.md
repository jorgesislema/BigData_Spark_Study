# ğŸ”„ Procesamiento de Streaming en Apache Spark

## ğŸ”¥ IntroducciÃ³n
El **procesamiento en streaming** permite analizar datos en tiempo real a medida que llegan desde fuentes como sensores, logs, redes sociales y bases de datos. Apache **Spark Streaming** y **Structured Streaming** proporcionan una forma eficiente de procesar estos datos con latencia baja y escalabilidad.

---

## ğŸ“Œ Â¿Por quÃ© usar Spark para Streaming?
âœ… **Baja latencia**: Procesamiento en tiempo casi real.
âœ… **Escalabilidad**: Funciona en clÃºsteres distribuidos.
âœ… **Compatibilidad**: Soporta Kafka, AWS Kinesis, Sockets, y mÃ¡s.
âœ… **IntegraciÃ³n con SQL y Machine Learning**.

---

## ğŸ› ï¸ ConfiguraciÃ³n del Entorno
### ğŸ”¹ Inicializar SparkSession para Streaming
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("StreamingSpark") \
    .getOrCreate()
```

---

## ğŸ”„ Lectura de Datos en Streaming
### ğŸ”¹ Leer datos desde un Socket (Ejemplo en localhost)
```python
df = spark.readStream \
    .format("socket") \
    .option("host", "localhost") \
    .option("port", 9999) \
    .load()
df.printSchema()
```

### ğŸ”¹ Leer datos desde Kafka
```python
df = spark.readStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("subscribe", "mi_topic") \
    .load()
df.selectExpr("CAST(value AS STRING)").show()
```

---

## ğŸ“ Procesamiento de Datos en Streaming
### ğŸ”¹ Transformaciones en Tiempo Real
```python
from pyspark.sql.functions import split, col

df_transformed = df.withColumn("palabras", split(col("value"), " "))
df_transformed.writeStream.outputMode("append").format("console").start()
```

### ğŸ”¹ Agregaciones con Ventanas de Tiempo
```python
from pyspark.sql.functions import window, count

df_grouped = df.groupBy(window(col("timestamp"), "10 minutes")).count()
df_grouped.writeStream.outputMode("complete").format("console").start()
```

---

## ğŸ’¾ Escritura de Resultados en Streaming
### ğŸ”¹ Guardar en Parquet
```python
df.writeStream \
    .format("parquet") \
    .option("path", "s3a://mi-bucket/output") \
    .option("checkpointLocation", "s3a://mi-bucket/checkpoints") \
    .start()
```

### ğŸ”¹ Guardar en Kafka
```python
df.writeStream \
    .format("kafka") \
    .option("kafka.bootstrap.servers", "localhost:9092") \
    .option("topic", "output_topic") \
    .option("checkpointLocation", "/tmp/checkpoints") \
    .start()
```

---

## âš¡ OptimizaciÃ³n y Buenas PrÃ¡cticas
âœ… **Usar `checkpointLocation`** para tolerancia a fallos.
âœ… **Configurar `trigger`** para optimizar la ejecuciÃ³n:
```python
df.writeStream.trigger(processingTime="5 seconds").start()
```
âœ… **Ajustar particiones en Kafka** para mejor paralelismo.

---

## ğŸ¯ ConclusiÃ³n
Apache **Spark Streaming** permite analizar datos en tiempo real con alta eficiencia. La correcta configuraciÃ³n de **fuentes, transformaciones y almacenamiento** es clave para garantizar un sistema estable y escalable. ğŸš€

