# ğŸ¤– Machine Learning en Apache Spark

## ğŸ”¥ IntroducciÃ³n
Apache **Spark MLlib** es la biblioteca de **Machine Learning** de Spark, diseÃ±ada para entrenar modelos a gran escala con procesamiento distribuido. Soporta algoritmos de clasificaciÃ³n, regresiÃ³n, clustering y reducciÃ³n de dimensionalidad.

---

## ğŸ“Œ Ventajas de Spark MLlib
âœ… **Escalabilidad**: Funciona en clÃºsteres grandes con procesamiento paralelo.
âœ… **Compatibilidad**: Soporta mÃºltiples formatos de datos.
âœ… **IntegraciÃ³n con Pipelines**: Permite estructurar el flujo de entrenamiento.
âœ… **OptimizaciÃ³n en Memoria**: Usa estructuras distribuidas para mejorar el rendimiento.

---

## ğŸ› ï¸ ConfiguraciÃ³n del Entorno
### ğŸ”¹ Inicializar SparkSession
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("ML_Spark") \
    .getOrCreate()
```

---

## ğŸ”„ Carga y Preprocesamiento de Datos
### ğŸ”¹ Cargar Datos desde un CSV
```python
from pyspark.sql.functions import col

df = spark.read.csv("/ruta/dataset.csv", header=True, inferSchema=True)
df = df.select(col("feature1"), col("feature2"), col("label"))
df.show()
```

### ğŸ”¹ Transformar Datos con `VectorAssembler`
```python
from pyspark.ml.feature import VectorAssembler

assembler = VectorAssembler(inputCols=["feature1", "feature2"], outputCol="features")
df = assembler.transform(df).select("features", "label")
```

---

## ğŸ¤– Entrenamiento de Modelos
### ğŸ”¹ RegresiÃ³n LogÃ­stica
```python
from pyspark.ml.classification import LogisticRegression

lr = LogisticRegression(featuresCol="features", labelCol="label")
modelo = lr.fit(df)
```

### ğŸ”¹ Ãrboles de DecisiÃ³n
```python
from pyspark.ml.classification import DecisionTreeClassifier

dt = DecisionTreeClassifier(featuresCol="features", labelCol="label")
modelo_dt = dt.fit(df)
```

### ğŸ”¹ Clustering con K-Means
```python
from pyspark.ml.clustering import KMeans

kmeans = KMeans(featuresCol="features", k=3)
modelo_km = kmeans.fit(df)
```

---

## ğŸ“Š EvaluaciÃ³n de Modelos
### ğŸ”¹ Calcular MÃ©tricas de PrecisiÃ³n
```python
from pyspark.ml.evaluation import MulticlassClassificationEvaluator

evaluator = MulticlassClassificationEvaluator(labelCol="label", metricName="accuracy")
precision = evaluator.evaluate(modelo.transform(df))
print(f"PrecisiÃ³n del modelo: {precision}")
```

---

## âš¡ OptimizaciÃ³n y Buenas PrÃ¡cticas
âœ… **Usar Pipelines** para simplificar el flujo de ML:
```python
from pyspark.ml import Pipeline
pipeline = Pipeline(stages=[assembler, lr])
modelo_pipeline = pipeline.fit(df)
```
âœ… **HiperparÃ¡metros con Cross Validation**:
```python
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
paramGrid = ParamGridBuilder().addGrid(lr.regParam, [0.1, 0.01]).build()
```

---

## ğŸ¯ ConclusiÃ³n
Apache **Spark MLlib** permite entrenar modelos de **Machine Learning** en grandes volÃºmenes de datos de manera distribuida. Su integraciÃ³n con Pipelines y herramientas de optimizaciÃ³n lo convierte en una excelente opciÃ³n para escalar modelos predictivos. ğŸš€

