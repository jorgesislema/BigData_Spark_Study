# ğŸ”„ ETL en Apache Spark

## ğŸ”¥ IntroducciÃ³n
El proceso **ETL (Extract, Transform, Load)** es esencial en el mundo del **Big Data** para mover y transformar grandes volÃºmenes de datos. Apache **Spark** es una de las herramientas mÃ¡s eficientes para realizar ETL de manera distribuida y escalable.

---

## ğŸ“Œ Â¿QuÃ© es ETL?
**ETL** consiste en tres fases principales:
1. **Extract (ExtracciÃ³n):** ObtenciÃ³n de datos desde mÃºltiples fuentes.
2. **Transform (TransformaciÃ³n):** Limpieza, enriquecimiento y agregaciÃ³n de datos.
3. **Load (Carga):** Almacenamiento de los datos procesados en un destino final (Data Warehouse, Lakehouse, etc.).

---

## ğŸ› ï¸ ImplementaciÃ³n de ETL con PySpark
### ğŸ”¹ 1. Inicializar SparkSession
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("ETL_Spark") \
    .getOrCreate()
```

---

### ğŸ”¹ 2. ExtracciÃ³n de Datos
Podemos extraer datos desde varias fuentes:
#### ğŸ“‚ **Desde un archivo CSV**
```python
df = spark.read.option("header", "true").csv("s3a://mi-bucket/datos.csv")
df.show()
```
#### ğŸ—„ï¸ **Desde una base de datos SQL**
```python
df = spark.read \
    .format("jdbc") \
    .option("url", "jdbc:mysql://localhost:3306/mi_bd") \
    .option("dbtable", "usuarios") \
    .option("user", "root") \
    .option("password", "password") \
    .load()
df.show()
```

---

### ğŸ”¹ 3. TransformaciÃ³n de Datos
#### ğŸ”¹ **Limpieza y Filtrado**
```python
df = df.dropna()  # Elimina valores nulos
df = df.filter(df["edad"] > 18)  # Filtra mayores de 18 aÃ±os
```

#### ğŸ”¹ **Agregaciones y CÃ¡lculos**
```python
from pyspark.sql.functions import col, avg

df_agg = df.groupBy("ciudad").agg(avg("salario").alias("salario_promedio"))
df_agg.show()
```

---

### ğŸ”¹ 4. Carga de Datos
#### ğŸ›ï¸ **Guardar en Parquet**
```python
df.write.mode("overwrite").parquet("s3a://mi-bucket/output.parquet")
```

#### ğŸ“Š **Guardar en una base de datos SQL**
```python
df.write \
    .format("jdbc") \
    .option("url", "jdbc:mysql://localhost:3306/mi_bd") \
    .option("dbtable", "usuarios_procesados") \
    .option("user", "root") \
    .option("password", "password") \
    .save()
```

---

## âš¡ OptimizaciÃ³n en ETL con Spark
âœ… **Usar formato Parquet** en lugar de CSV para mejorar la compresiÃ³n y velocidad.
âœ… **Ajustar el nÃºmero de particiones** para optimizar el rendimiento:
```python
df = df.repartition(10)
```
âœ… **Cachear datos** si se reutilizan en mÃºltiples pasos:
```python
df.cache()
```

---

## ğŸ¯ ConclusiÃ³n
Apache **Spark** es una soluciÃ³n potente para procesos ETL en **Big Data**, permitiendo mover, transformar y almacenar datos de manera eficiente. Ajustar configuraciones y buenas prÃ¡cticas mejora significativamente el rendimiento. ğŸš€

