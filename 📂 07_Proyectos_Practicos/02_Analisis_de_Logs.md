# ğŸ“œ AnÃ¡lisis de Logs con Apache Spark

## ğŸ”¥ IntroducciÃ³n
El anÃ¡lisis de **logs** es una tarea esencial en la administraciÃ³n de sistemas, seguridad informÃ¡tica y optimizaciÃ³n de aplicaciones. Apache **Spark** permite procesar y analizar grandes volÃºmenes de logs de manera eficiente y en tiempo real.

---

## ğŸ“Œ Â¿Por quÃ© usar Spark para AnÃ¡lisis de Logs?
âœ… **Escalabilidad**: Puede manejar logs de mÃºltiples fuentes distribuidas.
âœ… **Velocidad**: Procesa datos en memoria y en paralelo.
âœ… **Flexibilidad**: Soporta logs en mÃºltiples formatos (JSON, CSV, texto plano, etc.).
âœ… **Streaming**: Permite anÃ¡lisis en tiempo real con **Spark Streaming**.

---

## ğŸ› ï¸ ConfiguraciÃ³n del Entorno
### ğŸ”¹ Inicializar SparkSession
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder \
    .appName("AnalisisLogs") \
    .getOrCreate()
```

---

## ğŸ”„ Carga de Logs en Spark
### ğŸ”¹ Leer un Archivo de Logs en Texto Plano
```python
df = spark.read.text("/ruta/logs/app.log")
df.show(truncate=False)
```

### ğŸ”¹ Leer Logs en JSON
```python
df_json = spark.read.json("/ruta/logs/logs.json")
df_json.show()
```

---

## ğŸ” Procesamiento y AnÃ¡lisis de Logs
### ğŸ”¹ Filtrar Errores en Logs
```python
error_logs = df.filter(df["value"].contains("ERROR"))
error_logs.show(truncate=False)
```

### ğŸ”¹ Extraer InformaciÃ³n con Expresiones Regulares
```python
from pyspark.sql.functions import regexp_extract

df = df.withColumn("timestamp", regexp_extract("value", r'(\d{4}-\d{2}-\d{2} \d{2}:\d{2}:\d{2})', 1))
df = df.withColumn("log_level", regexp_extract("value", r'(INFO|ERROR|WARN|DEBUG)', 1))
df.show()
```

### ğŸ”¹ Contar Frecuencia de Tipos de Logs
```python
from pyspark.sql.functions import count

df_count = df.groupBy("log_level").agg(count("log_level").alias("count"))
df_count.show()
```

---

## ğŸ“Š VisualizaciÃ³n de Datos
Spark permite exportar los datos a formatos adecuados para anÃ¡lisis visual con herramientas de BI.
```python
df.write.mode("overwrite").parquet("/ruta/output/logs_analizados.parquet")
```

---

## âš¡ OptimizaciÃ³n y Buenas PrÃ¡cticas
âœ… **Usar formato Parquet** en lugar de texto plano para optimizar almacenamiento y consultas.
âœ… **Ajustar el nÃºmero de particiones** para mejorar rendimiento:
```python
df = df.repartition(10)
```
âœ… **Cachear DataFrames** cuando se reutilicen en mÃºltiples consultas:
```python
df.cache()
```

---

## ğŸ¯ ConclusiÃ³n
Apache **Spark** facilita el anÃ¡lisis de logs a gran escala, permitiendo la detecciÃ³n de errores, patrones y mÃ©tricas en sistemas distribuidos. Implementar optimizaciones adecuadas mejora el rendimiento y la escalabilidad del procesamiento. ğŸš€

