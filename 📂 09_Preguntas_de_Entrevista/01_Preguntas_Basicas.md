# â“ 150 Preguntas BÃ¡sicas sobre Apache Spark

## ğŸ”¥ IntroducciÃ³n
Apache Spark es una de las tecnologÃ­as mÃ¡s utilizadas en **Big Data** y **procesamiento distribuido**. A continuaciÃ³n, se presentan **150 preguntas bÃ¡sicas** para entrevistas y aprendizaje con sus respectivas respuestas.

---

## ğŸ“Œ Preguntas Generales
1. **Â¿QuÃ© es Apache Spark?**  
   Apache Spark es un motor de procesamiento de datos de cÃ³digo abierto diseÃ±ado para el procesamiento distribuido y en memoria.

2. **Â¿CuÃ¡les son las principales caracterÃ­sticas de Spark?**  
   - Procesamiento en memoria
   - API en mÃºltiples lenguajes
   - Compatibilidad con SQL, ML y Streaming
   - Alta escalabilidad

3. **Â¿En quÃ© se diferencia Spark de Hadoop MapReduce?**  
   Spark es mucho mÃ¡s rÃ¡pido porque procesa datos en memoria, mientras que Hadoop usa disco para cada operaciÃ³n intermedia.

4. **Â¿CuÃ¡les son los componentes principales de Apache Spark?**  
   - Spark Core
   - Spark SQL
   - Spark Streaming
   - MLlib (Machine Learning)
   - GraphX (Procesamiento de grafos)

5. **Â¿QuÃ© lenguajes de programaciÃ³n son compatibles con Spark?**  
   Python, Scala, Java y R.

6. **Â¿QuÃ© es Spark Core?**  
   Es el motor central de Spark que maneja tareas de procesamiento distribuido y gestiÃ³n de memoria.

7. **Â¿CuÃ¡les son las bibliotecas adicionales que ofrece Spark?**  
   Spark SQL, Spark Streaming, MLlib y GraphX.

8. **Â¿QuÃ© ventajas tiene Spark sobre otras soluciones de procesamiento de datos?**  
   - Procesamiento en memoria
   - Velocidad superior
   - Soporte para mÃºltiples cargas de trabajo
   - IntegraciÃ³n con herramientas de Big Data

9. **Â¿CuÃ¡l es la Ãºltima versiÃ³n estable de Apache Spark?**  
   Se debe consultar la documentaciÃ³n oficial en [Apache Spark Releases](https://spark.apache.org/downloads.html).

10. **Â¿QuiÃ©n desarrollÃ³ Apache Spark y cuÃ¡ndo?**  
   Spark fue desarrollado por **AMPLab de UC Berkeley** en 2009 y donado a la Apache Software Foundation en 2013.

---

## âš¡ Arquitectura y Funcionamiento
11. **Â¿QuÃ© es un RDD en Spark?**  
    Un **RDD (Resilient Distributed Dataset)** es una colecciÃ³n inmutable y distribuida de elementos que se pueden procesar en paralelo en un clÃºster de Spark.

12. **Â¿CuÃ¡les son los tipos de RDDs en Spark?**  
    - **RDD por transformaciÃ³n de datos:** Creado a partir de otros RDDs mediante operaciones como `map()` o `filter()`.
    - **RDD por entrada de datos:** Creado a partir de fuentes externas como HDFS, S3 o bases de datos.

13. **Â¿CÃ³mo se crean los RDDs en Spark?**  
    - A partir de colecciones en el cÃ³digo (`parallelize`).
    - Desde fuentes externas como HDFS, S3 o bases de datos.
    - Mediante transformaciones en otros RDDs.

14. **Â¿QuÃ© significa la inmutabilidad en RDDs?**  
    Significa que una vez creado un RDD, no se puede modificar. Cualquier cambio genera un nuevo RDD.

15. **Â¿CuÃ¡les son las operaciones bÃ¡sicas en RDDs?**  
    - **Transformaciones:** `map()`, `flatMap()`, `filter()`, `reduceByKey()`.
    - **Acciones:** `collect()`, `count()`, `take()`, `saveAsTextFile()`.

16. **Â¿QuÃ© es un DAG en Spark?**  
    Un **DAG (Directed Acyclic Graph)** es una representaciÃ³n grÃ¡fica de la secuencia de operaciones que se ejecutarÃ¡n en un trabajo de Spark.

17. **Â¿CÃ³mo maneja Spark la tolerancia a fallos?**  
    - Utiliza **RDD Lineage**, lo que permite reconstruir un RDD en caso de fallo.
    - Usa replicaciÃ³n de datos en caso de ejecuciÃ³n en modo distribuido.

18. **Â¿QuÃ© es un ejecutor en Spark?**  
    Un **ejecutor** es un proceso que ejecuta tareas en un clÃºster de Spark y almacena datos en cachÃ© si es necesario.

19. **Â¿CuÃ¡l es la diferencia entre Spark Driver y Spark Executor?**  
    - **Spark Driver:** Coordina la ejecuciÃ³n de las tareas y transforma el DAG en tareas distribuidas.
    - **Spark Executor:** Ejecuta las tareas asignadas por el Driver y gestiona la memoria en cachÃ©.

20. **Â¿CÃ³mo funciona el paralelismo en Spark?**  
    - Spark divide los datos en **particiones**, que son procesadas en paralelo por mÃºltiples ejecutores.
    - Cada ejecutor ejecuta varias **tareas** en paralelo, optimizando el uso de los recursos del clÃºster.

...

# ## ğŸ”„ Transformaciones y Acciones

21. **Â¿QuÃ© son las transformaciones en Spark?**  
    Son operaciones que generan un nuevo RDD o DataFrame sin modificar el original. Son **perezosas**, lo que significa que no se ejecutan hasta que una acciÃ³n las activa. Ejemplos: `map()`, `filter()`, `flatMap()`.

22. **Â¿QuÃ© son las acciones en Spark?**  
    Son operaciones que devuelven un resultado al driver o escriben datos en almacenamiento. Activan la ejecuciÃ³n de las transformaciones. Ejemplos: `collect()`, `count()`, `take()`, `saveAsTextFile()`.

23. **Â¿CuÃ¡l es la diferencia entre transformaciones perezosas y acciones?**  
    Las **transformaciones** son perezosas y solo se computan cuando se llama a una **acciÃ³n**, lo que permite optimizar la ejecuciÃ³n y evitar cÃ¡lculos innecesarios.

24. **Â¿QuÃ© es `map()` y `flatMap()` en Spark?**  
    - `map()`: Aplica una funciÃ³n a cada elemento y devuelve un nuevo RDD/DataFrame con los resultados.
    - `flatMap()`: Similar a `map()`, pero aplana los resultados si la funciÃ³n devuelve mÃºltiples valores.

25. **Â¿QuÃ© hace la funciÃ³n `filter()` en Spark?**  
    Devuelve un nuevo RDD o DataFrame con los elementos que cumplen una condiciÃ³n especÃ­fica.

26. **Â¿CÃ³mo funciona `reduceByKey()`?**  
    Agrupa elementos por clave y aplica una funciÃ³n de reducciÃ³n en cada grupo, minimizando el shuffle.

27. **Â¿QuÃ© es `groupByKey()` y por quÃ© se debe evitar?**  
    `groupByKey()` agrupa valores por clave, pero **requiere mÃ¡s memoria y puede generar mÃ¡s trÃ¡fico de red** que `reduceByKey()`, por lo que se recomienda evitarlo en grandes volÃºmenes de datos.

28. **Â¿QuÃ© diferencia hay entre `repartition()` y `coalesce()`?**  
    - `repartition(n)`: Redistribuye los datos en `n` particiones, generando un shuffle.
    - `coalesce(n)`: Reduce el nÃºmero de particiones **sin shuffle** cuando es posible, por lo que es mÃ¡s eficiente.

29. **Â¿QuÃ© es `persist()` y cuÃ¡ndo se usa?**  
    Almacena un RDD/DataFrame en memoria o disco para reutilizaciÃ³n, evitando recomputaciones innecesarias.

30. **Â¿CuÃ¡l es la diferencia entre `cache()` y `persist()`?**  
    - `cache()`: Equivalente a `persist(StorageLevel.MEMORY_ONLY)`, almacena en memoria.
    - `persist(level)`: Permite definir el nivel de almacenamiento (memoria, disco o ambos).

---

## ğŸ“Š Spark SQL

31. **Â¿QuÃ© es Spark SQL?**  
    Es un mÃ³dulo de Apache Spark que permite el procesamiento de datos estructurados mediante consultas SQL o la API de DataFrames/Datasets.

32. **Â¿QuÃ© son los DataFrames en Spark?**  
    Son estructuras tabulares similares a tablas de bases de datos, optimizadas para trabajar con grandes volÃºmenes de datos.

33. **Â¿CÃ³mo se diferencian los DataFrames de los RDDs?**  
    - **RDDs:** MÃ¡s flexibles pero requieren mÃ¡s cÃ³digo para manipulaciones complejas.
    - **DataFrames:** MÃ¡s optimizados, usan el **Catalyst Optimizer** y almacenan datos en formato de columnas.

34. **Â¿QuÃ© es un Dataset en Spark?**  
    Es una estructura similar a un DataFrame pero con **tipado fuerte**, permitiendo mayor seguridad en el cÃ³digo.

35. **Â¿CÃ³mo se crea un DataFrame en Spark?**  
    - A partir de un archivo CSV, JSON o Parquet.
    - Desde una consulta SQL.
    - A partir de un RDD con `spark.createDataFrame()`.

36. **Â¿CÃ³mo se puede ejecutar SQL en Spark?**  
    Mediante `spark.sql("SELECT * FROM tabla")` o registrando un DataFrame como tabla temporal.

37. **Â¿QuÃ© es un `TempView` en Spark?**  
    Una vista temporal que permite ejecutar SQL sobre un DataFrame dentro de una sesiÃ³n de Spark.

38. **Â¿QuÃ© diferencia hay entre `createOrReplaceTempView()` y `createGlobalTempView()`?**  
    - `createOrReplaceTempView()`: Solo estÃ¡ disponible en la sesiÃ³n actual.
    - `createGlobalTempView()`: Disponible en todas las sesiones de Spark.

39. **Â¿CÃ³mo se puede leer un archivo Parquet en Spark?**  
    ```python
    df = spark.read.parquet("ruta/del/archivo.parquet")
    ```

40. **Â¿QuÃ© es el Optimizer Catalyst en Spark?**  
    Es el optimizador de consultas de Spark SQL, encargado de transformar y optimizar las consultas para mejorar su rendimiento.

---
## ğŸ”€ Spark Streaming

41. **Â¿QuÃ© es Spark Streaming?**  
    Es un mÃ³dulo de Spark que permite el procesamiento en tiempo real de flujos de datos provenientes de fuentes como Kafka, HDFS o sockets.

42. **Â¿CuÃ¡l es la diferencia entre Spark Streaming y Structured Streaming?**  
    - **Spark Streaming** trabaja con microbatches.
    - **Structured Streaming** usa un enfoque basado en consultas continuas con procesamiento optimizado.

43. **Â¿QuÃ© es un DStream en Spark Streaming?**  
    Es una secuencia de RDDs que representan flujos de datos en Spark Streaming.

44. **Â¿CÃ³mo se puede conectar Spark Streaming con Kafka?**  
    Usando `spark.readStream.format("kafka")` para consumir mensajes desde un tÃ³pico de Kafka.

45. **Â¿QuÃ© es `checkpointing` en Spark Streaming?**  
    Es una tÃ©cnica para almacenar estados intermedios y permitir la recuperaciÃ³n en caso de fallo.

46. **Â¿CÃ³mo se maneja la latencia en Spark Streaming?**  
    Ajustando el intervalo de microbatches y optimizando la paralelizaciÃ³n.

47. **Â¿CuÃ¡les son los modos de salida en Structured Streaming?**  
    - **Append:** Solo agrega nuevos registros.
    - **Complete:** Reescribe toda la tabla.
    - **Update:** Actualiza solo los registros modificados.

48. **Â¿CÃ³mo se pueden gestionar eventos tardÃ­os en Structured Streaming?**  
    Usando `watermark()` para definir un tiempo de tolerancia a retrasos.

49. **Â¿QuÃ© es `watermark()` en Structured Streaming?**  
    Es un mecanismo para definir cuÃ¡nto tiempo se espera eventos tardÃ­os antes de descartarlos.

50. **Â¿CÃ³mo se configura un flujo de datos en Spark Streaming?**  
    ```python
    df = spark.readStream.format("socket").option("host", "localhost").option("port", 9999).load()
    ```

---


51. **Â¿QuÃ© es el Broadcast Join en Spark?**  
    Es una tÃ©cnica para mejorar el rendimiento de los joins cuando una de las tablas es pequeÃ±a, transmitiÃ©ndola a todos los ejecutores para evitar el shuffle.

52. **Â¿CuÃ¡ndo se debe usar `broadcast()` en joins?**  
    Se debe usar cuando una tabla es lo suficientemente pequeÃ±a como para caber en la memoria de cada nodo del clÃºster, reduciendo la sobrecarga del shuffle.

53. **Â¿QuÃ© es el Shuffle en Spark?**  
    Es el proceso de redistribuciÃ³n de datos entre particiones, generalmente causado por operaciones como `groupBy()`, `reduceByKey()` y `join()`.

54. **Â¿CÃ³mo se puede optimizar el shuffle en Spark?**  
    - Usar `reduceByKey()` en lugar de `groupByKey()`.
    - Ajustar `spark.sql.shuffle.partitions`.
    - Usar `broadcast()` para joins con tablas pequeÃ±as.
    - Configurar `coalesce()` para reducir particiones sin shuffle.

55. **Â¿QuÃ© es `spark.sql.shuffle.partitions` y cÃ³mo afecta el rendimiento?**  
    Es el nÃºmero de particiones que se usan en operaciones de shuffle en Spark SQL. Un valor alto mejora la paralelizaciÃ³n, pero un valor excesivo puede generar sobrecarga en la gestiÃ³n de tareas.

56. **Â¿QuÃ© es AQE (Adaptive Query Execution) en Spark?**  
    Es una funcionalidad que ajusta dinÃ¡micamente el plan de ejecuciÃ³n en funciÃ³n de la ejecuciÃ³n en tiempo real, optimizando el nÃºmero de particiones y la estrategia de joins.

57. **Â¿CÃ³mo se puede mejorar la ejecuciÃ³n de consultas en Spark SQL?**  
    - Usar **particionamiento adecuado** en los datos.
    - Habilitar AQE (`spark.sql.adaptive.enabled = true`).
    - Aplicar **pruning** en la consulta (`filter` antes de la agregaciÃ³n).
    - Usar formatos eficientes como **Parquet**.

58. **Â¿CÃ³mo se puede reducir el uso de memoria en Spark?**  
    - Usar `persist(StorageLevel.DISK_ONLY)` para evitar consumir RAM.
    - Evitar `collect()` en grandes volÃºmenes de datos.
    - Ajustar `spark.memory.fraction` y `spark.memory.storageFraction`.

59. **Â¿CÃ³mo se puede depurar un trabajo en Spark?**  
    - Revisar logs en la UI de Spark.
    - Usar `explain()` para analizar el plan de ejecuciÃ³n.
    - Habilitar `spark.eventLog.enabled` para registrar eventos y analizar fallos.



## ğŸ”¥ OptimizaciÃ³n en Spark

60. **Â¿QuÃ© herramientas se pueden usar para monitorear un trabajo de Spark?**  
    - **Spark UI**: Monitoreo en tiempo real del DAG y tareas.
    - **Ganglia y Prometheus**: MÃ©tricas avanzadas.
    - **Event Logs**: Registro detallado del historial de ejecuciÃ³n.

61. **Â¿CÃ³mo manejar la concurrencia en Spark?**  
    - Ajustando `spark.sql.shuffle.partitions` y `spark.task.cpus`.

62. **Â¿QuÃ© impacto tiene el Garbage Collection en Spark?**  
    - Puede ralentizar las tareas si hay demasiados objetos en memoria.

63. **Â¿CÃ³mo manejar datos sesgados en Spark?**  
    - Usando tÃ©cnicas como **salting** o `skew join optimization`.

64. **Â¿QuÃ© es la serializaciÃ³n en Spark y cÃ³mo optimizarla?**  
    - Configurar `spark.serializer` para usar `KryoSerializer` en vez del predeterminado.

65. **Â¿CÃ³mo configurar Spark para ejecuciÃ³n en Kubernetes?**  
    - Definiendo `spark.kubernetes.container.image` y `spark.kubernetes.namespace`.

66. **Â¿CÃ³mo prevenir cuellos de botella en Spark?**  
    - Optimizando el paralelismo y evitando grandes acciones como `collect()`.

67. **Â¿QuÃ© es un Stage en Spark?**  
    - Una unidad de ejecuciÃ³n de tareas generada por el DAG Scheduler.

68. **Â¿CÃ³mo mejorar el rendimiento en ETLs con Spark?**  
    - Usar `coalesce()` para reducir el nÃºmero de archivos pequeÃ±os.

69. **Â¿CÃ³mo evitar el re-procesamiento en Structured Streaming?**  
    - Usando `checkpointing` y `watermark()`.

70. **Â¿CÃ³mo optimizar joins en tablas grandes en Spark?**  
    - Usar `sort-merge join` en lugar de `broadcast join`.

71. **Â¿QuÃ© es el cost-based optimizer (CBO) en Spark?**  
    - Un optimizador que mejora los planes de ejecuciÃ³n basado en estadÃ­sticas de los datos.

72. **Â¿CÃ³mo funciona la recolecciÃ³n de estadÃ­sticas en Spark?**  
    - Ejecutando `ANALYZE TABLE table_name COMPUTE STATISTICS`.

73. **Â¿CÃ³mo mejorar el rendimiento de un clÃºster de Spark?**  
    - Ajustando `spark.dynamicAllocation.enabled` y `spark.executor.instances`.

74. **Â¿CÃ³mo manejar datos faltantes en Spark?**  
    - Usando `fillna()`, `dropna()` o `replace()` en DataFrames.

75. **Â¿CÃ³mo mejorar la eficiencia en consultas con filtros?**  
    - Aplicando `predicate pushdown` en la lectura de archivos.

76. **Â¿CÃ³mo optimizar el uso de CPU en Spark?**  
    - Ajustando `spark.task.cpus` y `spark.executor.cores`.

77. **Â¿QuÃ© es `spark.memory.offHeap.enabled` y cuÃ¡ndo usarlo?**  
    - Permite gestionar memoria fuera del heap de JVM, Ãºtil en grandes cargas de trabajo.

78. **Â¿CÃ³mo se optimiza el uso de cachÃ© en Spark?**  
    - Usando `persist()` en niveles estratÃ©gicos y evitando `cache()` innecesario.

79. **Â¿CÃ³mo reducir la sobrecarga de DAG en Spark?**  
    - Usando `checkpoint()` para cortar la dependencia de linaje.

80. **Â¿QuÃ© es un task en Spark?**  
    - Una unidad de ejecuciÃ³n individual dentro de un executor.

81. **Â¿CÃ³mo ajustar `spark.default.parallelism` en Spark?**  
    - DefiniÃ©ndolo manualmente en base a la capacidad del clÃºster.

82. **Â¿CÃ³mo Spark maneja el backpressure en Structured Streaming?**  
    - Ajustando `spark.streaming.backpressure.enabled`.

83. **Â¿QuÃ© es la compactaciÃ³n en Spark?**  
    - Un proceso que fusiona pequeÃ±os archivos en uno mÃ¡s grande para mejorar el rendimiento de lectura.

84. **Â¿CÃ³mo manejar mÃºltiples fuentes de datos en Spark?**  
    - Usando `union()` y `join()` para combinar datasets.

85. **Â¿CÃ³mo evitar el re-procesamiento en cargas incrementales en Spark?**  
    - Aplicando `watermark()` y `checkpointing` en Structured Streaming.

86. **Â¿CÃ³mo Spark maneja la recuperaciÃ³n ante fallos?**  
    - Mediante la re-ejecuciÃ³n de tareas fallidas y uso de DAG lineage.

87. **Â¿CÃ³mo configurar Spark en entornos con restricciones de memoria?**  
    - Ajustando `spark.memory.fraction` y `spark.memory.storageFraction`.

88. **Â¿QuÃ© es el `TaskScheduler` en Spark?**  
    - Un componente que asigna tareas a los ejecutores disponibles.

89. **Â¿CÃ³mo evitar la sobrecarga de logs en Spark?**  
    - Configurando `log4j.properties` y reduciendo el nivel de logs.

90. **Â¿CÃ³mo optimizar el manejo de grandes volÃºmenes de datos en Spark?**  
    - Usando **bucketing** y **partitioning** en almacenamiento distribuido.

91. **Â¿CÃ³mo mejorar el rendimiento en consultas sobre grandes conjuntos de datos en Spark?**

    - Aplicando predicate pushdown y pruning de columnas.

91. **Â¿CÃ³mo evitar la sobrecarga de shuffle en Spark?**

    - Usando tÃ©cnicas como coalesce(), reduceByKey() y broadcast() en joins.

92. **Â¿QuÃ© es el almacenamiento en cachÃ© en Spark y cuÃ¡ndo debe usarse?**

    - Permite reutilizar datos en memoria sin necesidad de recÃ¡lculo, Ãºtil para consultas repetitivas.

93. **Â¿CÃ³mo optimizar la ejecuciÃ³n de Spark en clÃºsteres con recursos limitados?**

    - Ajustando spark.executor.memory, spark.executor.cores y activando dynamic allocation.

94. **Â¿CÃ³mo se puede mejorar la escalabilidad en Spark?**

    - Ajustando la cantidad de particiones, ejecutores y recursos dinÃ¡micos.

95. **Â¿QuÃ© impacto tiene el nÃºmero de particiones en el rendimiento de Spark?**

    - Un nÃºmero bajo de particiones reduce el paralelismo, mientras que demasiadas particiones pueden generar overhead en la gestiÃ³n de tareas.

96. **Â¿CÃ³mo minimizar el tiempo de ejecuciÃ³n en Spark?**

    - Aplicando Adaptive Query Execution (AQE) y optimizando la planificaciÃ³n de DAG.

97. **Â¿CÃ³mo identificar cuellos de botella en Spark?**

    - Usando Spark UI, revisando Stages y Tasks y analizando los tiempos de ejecuciÃ³n.

98. **Â¿CÃ³mo evitar la fragmentaciÃ³n de archivos en Spark?**

    - Ajustando el tamaÃ±o de particiones antes de escribir, usando coalesce() o repartition().

99. **Â¿CÃ³mo evitar la fragmentaciÃ³n de archivos en Spark?**

    - Ajustando el tamaÃ±o de particiones antes de escribir, usando coalesce() o repartition().

100. **Â¿CÃ³mo se gestiona la paralelizaciÃ³n en Spark?**
    - Configurando adecuadamente el nÃºmero de particiones, tareas y recursos del clÃºster.

