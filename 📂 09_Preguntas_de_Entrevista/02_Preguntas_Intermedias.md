## ğŸ”¥ Preguntas Intermedias sobre Apache Spark

### ğŸ“Œ General

1. **Â¿QuÃ© es Apache Spark y para quÃ© se utiliza?**  
   Apache Spark es un motor de procesamiento de datos de cÃ³digo abierto, diseÃ±ado para procesar grandes volÃºmenes de datos en paralelo de manera rÃ¡pida y eficiente. Se utiliza en tareas de anÃ¡lisis de datos, machine learning, procesamiento en tiempo real y Big Data.

2. **Â¿CuÃ¡les son los principales mÃ³dulos de Apache Spark?**  
   - **Spark Core**: Maneja la ejecuciÃ³n y administraciÃ³n de tareas.
   - **Spark SQL**: Permite consultas SQL sobre datos estructurados.
   - **Spark Streaming**: Procesamiento de datos en tiempo real.
   - **MLlib**: LibrerÃ­a de Machine Learning.
   - **GraphX**: Procesamiento de datos en grafos.

3. **Â¿CÃ³mo se diferencia Spark de MapReduce en tÃ©rminos de rendimiento?**  
   Spark es significativamente mÃ¡s rÃ¡pido que MapReduce porque procesa los datos en memoria, mientras que MapReduce escribe los resultados intermedios en disco, lo que genera una gran latencia.

4. **Â¿QuÃ© ventajas ofrece Spark sobre otras tecnologÃ­as de procesamiento de datos?**  
   - Procesamiento en memoria para mayor velocidad.
   - API en mÃºltiples lenguajes (Scala, Python, Java, R).
   - Soporte para procesamiento por lotes y en tiempo real.
   - Compatibilidad con Hadoop y otras herramientas de Big Data.

5. **Â¿CuÃ¡les son las desventajas o limitaciones de Spark?**  
   - Alto consumo de memoria.
   - Puede ser mÃ¡s costoso en la nube debido a la necesidad de hardware mÃ¡s potente.
   - Requiere ajustes para optimizar el rendimiento en grandes clÃºsteres.

6. **Â¿En quÃ© escenarios es mÃ¡s Ãºtil usar Spark en lugar de Hadoop?**  
   - Cuando se requiere procesamiento en tiempo real o anÃ¡lisis en memoria.
   - Para ejecutar cargas de trabajo iterativas como Machine Learning.
   - En entornos donde se necesita flexibilidad con diferentes fuentes de datos.

7. **Â¿QuÃ© es el SparkContext y por quÃ© es importante?**  
   Es el punto de entrada principal para cualquier aplicaciÃ³n de Spark. Administra la configuraciÃ³n de la aplicaciÃ³n y la distribuciÃ³n de tareas en el clÃºster.

8. **Â¿CÃ³mo se inicia una sesiÃ³n de Spark?**  
   Se inicia creando un `SparkSession` en cÃ³digo o ejecutando `spark-shell` en la lÃ­nea de comandos.

9. **Â¿QuÃ© es un SparkSession y cuÃ¡l es su funciÃ³n?**  
   Es el punto de entrada unificado para trabajar con Spark, reemplazando la necesidad de `SparkContext`, `SQLContext` y `HiveContext`.

10. **Â¿CuÃ¡les son los principales entornos donde se puede ejecutar Spark?**  
    - Modo local en una sola mÃ¡quina.
    - ClÃºster Hadoop YARN.
    - Apache Mesos.
    - Kubernetes.
    - Servicios en la nube como AWS EMR, Databricks, Google Cloud Dataproc.

### âš¡ Arquitectura y Funcionamiento

11. **Â¿QuÃ© es un RDD y cÃ³mo se diferencia de un DataFrame?**  
    - **RDD (Resilient Distributed Dataset)**: Estructura fundamental en Spark, basada en datos distribuidos y procesamiento inmutable.
    - **DataFrame**: ColecciÃ³n de datos organizados en formato tabular, optimizados con Catalyst Optimizer y mÃ¡s fÃ¡ciles de usar que los RDDs.

12. **Â¿CÃ³mo se crean y transforman los RDDs en Spark?**  
    - Desde colecciones locales usando `parallelize()`.
    - Desde archivos almacenados en HDFS, S3, etc.
    - Aplicando transformaciones como `map()`, `filter()`, `flatMap()`.

13. **Â¿QuÃ© operaciones se pueden realizar en un RDD?**  
    - **Transformaciones**: `map()`, `filter()`, `reduceByKey()`.
    - **Acciones**: `count()`, `collect()`, `take()`.

14. **Â¿QuÃ© es el Lazy Evaluation en Spark y por quÃ© es importante?**  
    Significa que las transformaciones en Spark no se ejecutan inmediatamente, sino que se construye un plan de ejecuciÃ³n que solo se activa cuando se realiza una acciÃ³n. Esto optimiza el rendimiento y reduce la sobrecarga de cÃ¡lculo.

15. **Â¿QuÃ© es el Directed Acyclic Graph (DAG) en Spark?**  
    Es un grÃ¡fico que representa la secuencia de operaciones en una tarea de Spark, optimizando la ejecuciÃ³n y eliminando redundancias.

16. **Â¿CÃ³mo funciona la planificaciÃ³n de tareas en Spark?**  
    - El Driver convierte las operaciones en un DAG.
    - Divide el DAG en Stages.
    - Distribuye tareas entre ejecutores en un clÃºster.

17. **Â¿QuÃ© papel juegan los ejecutores en la ejecuciÃ³n de tareas?**  
    Son procesos que ejecutan tareas asignadas por el Driver y administran la memoria para el almacenamiento en cachÃ©.

18. **Â¿CÃ³mo maneja Spark la tolerancia a fallos?**  
    - **Lineage de RDDs**: Permite reconstruir datos a partir de transformaciones previas.
    - **ReplicaciÃ³n en memoria y disco**.
    - **ReejecuciÃ³n de tareas fallidas**.

19. **Â¿QuÃ© es la ejecuciÃ³n especulativa en Spark y cuÃ¡ndo se usa?**  
    Es una funciÃ³n que detecta tareas lentas y las reejecuta en otros nodos para evitar cuellos de botella.

20. **Â¿CÃ³mo se optimiza el nÃºmero de particiones en un trabajo de Spark?**  
    - Ajustando `repartition(n)` o `coalesce(n)` segÃºn la carga de datos.
    - Configurando `spark.default.parallelism` en base al nÃºmero de nÃºcleos disponibles.

### ğŸ”„ Transformaciones y Acciones

21. **Â¿CuÃ¡l es la diferencia entre transformaciones y acciones en Spark?**  
    - **Transformaciones**: Devuelven un nuevo RDD/DataFrame sin ejecutarse inmediatamente. Ejemplo: `map()`, `filter()`, `groupByKey()`.
    - **Acciones**: Ejecutan las transformaciones y devuelven un resultado al driver. Ejemplo: `count()`, `collect()`, `take()`.

22. **Â¿QuÃ© es un narrow transformation y un wide transformation?**  
    - **Narrow Transformation**: Las operaciones afectan solo una particiÃ³n, sin requerir redistribuciÃ³n de datos. Ejemplo: `map()`, `filter()`.
    - **Wide Transformation**: Requiere redistribuir datos entre mÃºltiples particiones (shuffle). Ejemplo: `groupByKey()`, `reduceByKey()`.

23. **Â¿CuÃ¡les son ejemplos de transformaciones comunes en Spark?**  
    - `map()`, `flatMap()`, `filter()`, `groupByKey()`, `reduceByKey()`, `repartition()`, `coalesce()`.

24. **Â¿CuÃ¡les son ejemplos de acciones comunes en Spark?**  
    - `count()`, `collect()`, `take()`, `first()`, `saveAsTextFile()`, `foreach()`.

25. **Â¿QuÃ© es el proceso de Shuffle en Spark?**  
    Es el proceso de redistribuciÃ³n de datos entre particiones, ocurre en operaciones como `groupByKey()` y `join()`, y puede afectar el rendimiento debido al trÃ¡fico de red.

26. **Â¿CÃ³mo se minimiza el Shuffle en Spark?**  
    - Usar `reduceByKey()` en lugar de `groupByKey()`.
    - Ajustar el nÃºmero de particiones con `coalesce()` en lugar de `repartition()`.
    - Evitar joins innecesarios y usar `broadcast()` cuando sea posible.

27. **Â¿CÃ³mo funcionan las transformaciones `map()` y `flatMap()`?**  
    - `map()`: Aplica una funciÃ³n a cada elemento del RDD/DataFrame y devuelve un solo valor por entrada.
    - `flatMap()`: Similar a `map()`, pero permite devolver mÃºltiples valores por entrada, resultando en una estructura aplanada.

28. **Â¿CÃ³mo se diferencian `groupByKey()` y `reduceByKey()`?**  
    - `groupByKey()`: Agrupa los valores por clave sin reducirlos, lo que puede generar un alto uso de memoria y shuffle.
    - `reduceByKey()`: Aplica una funciÃ³n de reducciÃ³n directamente en cada clave antes del shuffle, mejorando el rendimiento.

29. **Â¿Por quÃ© `reduceByKey()` es mÃ¡s eficiente que `groupByKey()`?**  
    `reduceByKey()` realiza la agregaciÃ³n localmente en cada particiÃ³n antes del shuffle, lo que minimiza la cantidad de datos transferidos a travÃ©s de la red.

30. **Â¿CÃ³mo se pueden unir diferentes conjuntos de datos en Spark?**  
    - Usando `join()` para combinar DataFrames/RDDs basados en una clave comÃºn.
    - Optimizando joins con `broadcast()` cuando una de las tablas es pequeÃ±a.

### ğŸ“Š Spark SQL y DataFrames

31. **Â¿QuÃ© es Spark SQL y para quÃ© se utiliza?**  
    Es un mÃ³dulo de Spark que permite consultar datos estructurados usando SQL y APIs de DataFrames/Datasets, proporcionando optimizaciones automÃ¡ticas a travÃ©s del **Catalyst Optimizer**.

32. **Â¿CuÃ¡l es la diferencia entre un DataFrame y un Dataset en Spark?**  
    - **DataFrame**: ColecciÃ³n de datos estructurados similar a una tabla SQL, sin tipado estricto.
    - **Dataset**: Similar a un DataFrame, pero con tipado fuerte y mÃ¡s seguridad en tiempo de compilaciÃ³n (solo disponible en Scala y Java).

33. **Â¿CÃ³mo se crean DataFrames en Spark?**  
    - Desde archivos CSV, JSON, Parquet: `spark.read.format("csv").load("archivo.csv")`.
    - A partir de una consulta SQL: `spark.sql("SELECT * FROM tabla")`.
    - Desde RDDs con `spark.createDataFrame(rdd, schema)`.

34. **Â¿CÃ³mo se puede ejecutar SQL en Spark?**  
    - Usando `spark.sql("SELECT * FROM tabla")`.
    - Creando una vista temporal con `createOrReplaceTempView()` y ejecutando consultas SQL sobre ella.

35. **Â¿QuÃ© es una vista temporal en Spark SQL?**  
    Es una vista lÃ³gica que permite consultar un DataFrame con SQL dentro de una sesiÃ³n de Spark, sin persistencia en disco.

36. **Â¿QuÃ© diferencia hay entre `createOrReplaceTempView()` y `createGlobalTempView()`?**  
    - `createOrReplaceTempView()`: Solo estÃ¡ disponible dentro de la sesiÃ³n de Spark actual.
    - `createGlobalTempView()`: Disponible en todas las sesiones de Spark dentro del clÃºster.

37. **Â¿CÃ³mo se pueden leer datos de diferentes formatos en Spark SQL?**  
    - CSV: `spark.read.csv("archivo.csv")`.
    - JSON: `spark.read.json("archivo.json")`.
    - Parquet: `spark.read.parquet("archivo.parquet")`.
    - ORC: `spark.read.orc("archivo.orc")`.

38. **Â¿QuÃ© es el Catalyst Optimizer en Spark?**  
    Es el optimizador de consultas de Spark SQL, encargado de transformar y mejorar el plan de ejecuciÃ³n de consultas para maximizar el rendimiento.

39. **Â¿CÃ³mo funciona el Predicate Pushdown en Spark?**  
    Es una tÃ©cnica de optimizaciÃ³n que filtra los datos lo mÃ¡s cerca posible de la fuente, reduciendo la cantidad de datos leÃ­dos y mejorando el rendimiento.

40. **Â¿CÃ³mo se pueden escribir resultados en diferentes formatos en Spark SQL?**  
    - CSV: `df.write.csv("salida.csv")`.
    - JSON: `df.write.json("salida.json")`.
    - Parquet: `df.write.parquet("salida.parquet")`.
    - ORC: `df.write.orc("salida.orc")`.

### ğŸ”€ Spark Streaming

41. **Â¿QuÃ© es Spark Streaming y en quÃ© se diferencia de Batch Processing?**  
    Spark Streaming permite procesar datos en tiempo real dividiÃ©ndolos en pequeÃ±os micro-batches, mientras que Batch Processing procesa datos en lotes estÃ¡ticos sin una entrada continua.

42. **Â¿CuÃ¡l es la diferencia entre Spark Streaming y Structured Streaming?**  
    - **Spark Streaming** usa **DStreams** (RDDs de datos en tiempo real).
    - **Structured Streaming** usa **DataFrames y Datasets**, lo que permite optimizaciones automÃ¡ticas con el Catalyst Optimizer y una sintaxis SQL mÃ¡s intuitiva.

43. **Â¿CÃ³mo se procesa un flujo de datos en Spark Streaming?**  
    1. **RecepciÃ³n** de datos en micro-batches desde una fuente como Kafka o sockets.
    2. **TransformaciÃ³n** aplicando operaciones como `map()`, `filter()`, `groupBy()`.
    3. **Salida** almacenando los resultados en bases de datos, sistemas de archivos o dashboards.

44. **Â¿QuÃ© son los DStreams en Spark Streaming?**  
    Son estructuras de datos en Spark Streaming que representan una serie de RDDs en tiempo real y permiten procesamiento distribuido de flujos de datos continuos.

45. **Â¿CÃ³mo se conecta Spark Streaming con Kafka?**  
    - Usando `spark.readStream.format("kafka")` en Structured Streaming.
    - Usando `KafkaUtils.createDirectStream()` en Spark Streaming tradicional.

46. **Â¿CÃ³mo se gestiona el checkpointing en Spark Streaming?**  
    El checkpointing guarda el estado de la aplicaciÃ³n en HDFS o S3 para recuperaciÃ³n en caso de fallos, asegurando tolerancia a fallos en el procesamiento de flujos.

47. **Â¿QuÃ© es el concepto de Watermarking en Structured Streaming?**  
    Es una tÃ©cnica para manejar eventos tardÃ­os en flujos de datos, estableciendo un lÃ­mite de tiempo hasta el cual Spark considera datos atrasados en una ventana de agregaciÃ³n.

48. **Â¿CÃ³mo se manejan eventos tardÃ­os en Spark Streaming?**  
    - Con **Watermarking** para establecer un umbral de retenciÃ³n de datos.
    - Con **ventanas de tiempo** (`window()` en Structured Streaming) para agrupar eventos en intervalos.

49. **Â¿QuÃ© diferencia hay entre Output Modes en Structured Streaming?**  
    - **Append**: Solo muestra nuevas filas.
    - **Complete**: Reemplaza toda la tabla con cada actualizaciÃ³n.
    - **Update**: Solo actualiza las filas modificadas.

50. **Â¿CÃ³mo se pueden almacenar los resultados de un streaming en Spark?**  
    - En sistemas de archivos (`writeStream.format("parquet").start()`).
    - En bases de datos (`writeStream.format("jdbc")`).
    - En Kafka (`writeStream.format("kafka")`).

### ğŸš€ OptimizaciÃ³n en Spark

51. **Â¿QuÃ© es Adaptive Query Execution (AQE) en Spark?**  
    Es una optimizaciÃ³n en tiempo de ejecuciÃ³n que ajusta dinÃ¡micamente particiones y estrategias de join segÃºn los datos reales procesados.

52. **Â¿CÃ³mo se puede mejorar el rendimiento de consultas en Spark SQL?**  
    - Usar formatos como Parquet y ORC.
    - Habilitar AQE (`spark.sql.adaptive.enabled = true`).
    - Aplicar filtrado temprano (`WHERE` antes de `JOIN`).

53. **Â¿QuÃ© son los Broadcast Joins y cuÃ¡ndo se usan?**  
    Son una optimizaciÃ³n donde Spark envÃ­a una tabla pequeÃ±a a todos los ejecutores en lugar de hacer un shuffle masivo, Ãºtil cuando una de las tablas es pequeÃ±a (`broadcast(df)`).

54. **Â¿CÃ³mo afecta el nÃºmero de particiones al rendimiento de Spark?**  
    - **Pocas particiones** pueden causar un uso ineficiente de los recursos.
    - **Demasiadas particiones** pueden aumentar la sobrecarga de administraciÃ³n de tareas.

55. **Â¿CÃ³mo optimizar un Shuffle en Spark?**  
    - Usar `reduceByKey()` en lugar de `groupByKey()`.
    - Reducir el nÃºmero de particiones (`coalesce()`).
    - Habilitar AQE (`spark.sql.adaptive.enabled = true`).

56. **Â¿QuÃ© es `spark.sql.shuffle.partitions` y cuÃ¡ndo ajustarlo?**  
    Es el nÃºmero de particiones usadas en operaciones de shuffle en Spark SQL. Se recomienda ajustar segÃºn la cantidad de datos y nÃºcleos disponibles (`spark.conf.set("spark.sql.shuffle.partitions", 200)`).

57. **Â¿CÃ³mo mejorar el rendimiento de escrituras en Spark?**  
    - Usar `partitionBy()` al escribir archivos grandes.
    - Aplicar `coalesce()` para reducir archivos pequeÃ±os.
    - Preferir formatos binarios eficientes como Parquet.

58. **Â¿CuÃ¡les son las mejores prÃ¡cticas para optimizar la memoria en Spark?**  
    - Usar `persist()` y `cache()` sabiamente.
    - Ajustar `spark.memory.fraction` y `spark.memory.storageFraction`.
    - Evitar `collect()` en grandes volÃºmenes de datos.

59. **Â¿CÃ³mo funciona el Garbage Collection en Spark?**  
    - Java GC maneja la memoria de Spark, lo que puede causar pausas.
    - Se puede optimizar con `spark.memory.fraction` y evitando objetos innecesarios en cachÃ©.

60. **Â¿CÃ³mo evitar la fragmentaciÃ³n de archivos al escribir datos en Spark?**  
    - Usar `coalesce()` para reducir la cantidad de archivos pequeÃ±os.
    - Configurar `spark.sql.files.maxPartitionBytes` para controlar el tamaÃ±o de cada particiÃ³n.



