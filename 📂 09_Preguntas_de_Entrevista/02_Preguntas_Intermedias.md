## ğŸ”¥ Preguntas Intermedias sobre Apache Spark

### ğŸ“Œ General

1. **Â¿QuÃ© es Apache Spark y para quÃ© se utiliza?**  
   Apache Spark es un motor de procesamiento de datos de cÃ³digo abierto, diseÃ±ado para procesar grandes volÃºmenes de datos en paralelo de manera rÃ¡pida y eficiente. Se utiliza en tareas de anÃ¡lisis de datos, machine learning, procesamiento en tiempo real y Big Data.

2. **Â¿CuÃ¡les son los principales mÃ³dulos de Apache Spark?**  
   - **Spark Core**: Maneja la ejecuciÃ³n y administraciÃ³n de tareas.
   - **Spark SQL**: Permite consultas SQL sobre datos estructurados.
   - **Spark Streaming**: Procesamiento de datos en tiempo real.
   - **MLlib**: LibrerÃ­a de Machine Learning.
   - **GraphX**: Procesamiento de datos en grafos.

3. **Â¿CÃ³mo se diferencia Spark de MapReduce en tÃ©rminos de rendimiento?**  
   Spark es significativamente mÃ¡s rÃ¡pido que MapReduce porque procesa los datos en memoria, mientras que MapReduce escribe los resultados intermedios en disco, lo que genera una gran latencia.

4. **Â¿QuÃ© ventajas ofrece Spark sobre otras tecnologÃ­as de procesamiento de datos?**  
   - Procesamiento en memoria para mayor velocidad.
   - API en mÃºltiples lenguajes (Scala, Python, Java, R).
   - Soporte para procesamiento por lotes y en tiempo real.
   - Compatibilidad con Hadoop y otras herramientas de Big Data.

5. **Â¿CuÃ¡les son las desventajas o limitaciones de Spark?**  
   - Alto consumo de memoria.
   - Puede ser mÃ¡s costoso en la nube debido a la necesidad de hardware mÃ¡s potente.
   - Requiere ajustes para optimizar el rendimiento en grandes clÃºsteres.

6. **Â¿En quÃ© escenarios es mÃ¡s Ãºtil usar Spark en lugar de Hadoop?**  
   - Cuando se requiere procesamiento en tiempo real o anÃ¡lisis en memoria.
   - Para ejecutar cargas de trabajo iterativas como Machine Learning.
   - En entornos donde se necesita flexibilidad con diferentes fuentes de datos.

7. **Â¿QuÃ© es el SparkContext y por quÃ© es importante?**  
   Es el punto de entrada principal para cualquier aplicaciÃ³n de Spark. Administra la configuraciÃ³n de la aplicaciÃ³n y la distribuciÃ³n de tareas en el clÃºster.

8. **Â¿CÃ³mo se inicia una sesiÃ³n de Spark?**  
   Se inicia creando un `SparkSession` en cÃ³digo o ejecutando `spark-shell` en la lÃ­nea de comandos.

9. **Â¿QuÃ© es un SparkSession y cuÃ¡l es su funciÃ³n?**  
   Es el punto de entrada unificado para trabajar con Spark, reemplazando la necesidad de `SparkContext`, `SQLContext` y `HiveContext`.

10. **Â¿CuÃ¡les son los principales entornos donde se puede ejecutar Spark?**  
    - Modo local en una sola mÃ¡quina.
    - ClÃºster Hadoop YARN.
    - Apache Mesos.
    - Kubernetes.
    - Servicios en la nube como AWS EMR, Databricks, Google Cloud Dataproc.

### âš¡ Arquitectura y Funcionamiento

11. **Â¿QuÃ© es un RDD y cÃ³mo se diferencia de un DataFrame?**  
    - **RDD (Resilient Distributed Dataset)**: Estructura fundamental en Spark, basada en datos distribuidos y procesamiento inmutable.
    - **DataFrame**: ColecciÃ³n de datos organizados en formato tabular, optimizados con Catalyst Optimizer y mÃ¡s fÃ¡ciles de usar que los RDDs.

12. **Â¿CÃ³mo se crean y transforman los RDDs en Spark?**  
    - Desde colecciones locales usando `parallelize()`.
    - Desde archivos almacenados en HDFS, S3, etc.
    - Aplicando transformaciones como `map()`, `filter()`, `flatMap()`.

13. **Â¿QuÃ© operaciones se pueden realizar en un RDD?**  
    - **Transformaciones**: `map()`, `filter()`, `reduceByKey()`.
    - **Acciones**: `count()`, `collect()`, `take()`.

14. **Â¿QuÃ© es el Lazy Evaluation en Spark y por quÃ© es importante?**  
    Significa que las transformaciones en Spark no se ejecutan inmediatamente, sino que se construye un plan de ejecuciÃ³n que solo se activa cuando se realiza una acciÃ³n. Esto optimiza el rendimiento y reduce la sobrecarga de cÃ¡lculo.

15. **Â¿QuÃ© es el Directed Acyclic Graph (DAG) en Spark?**  
    Es un grÃ¡fico que representa la secuencia de operaciones en una tarea de Spark, optimizando la ejecuciÃ³n y eliminando redundancias.

16. **Â¿CÃ³mo funciona la planificaciÃ³n de tareas en Spark?**  
    - El Driver convierte las operaciones en un DAG.
    - Divide el DAG en Stages.
    - Distribuye tareas entre ejecutores en un clÃºster.

17. **Â¿QuÃ© papel juegan los ejecutores en la ejecuciÃ³n de tareas?**  
    Son procesos que ejecutan tareas asignadas por el Driver y administran la memoria para el almacenamiento en cachÃ©.

18. **Â¿CÃ³mo maneja Spark la tolerancia a fallos?**  
    - **Lineage de RDDs**: Permite reconstruir datos a partir de transformaciones previas.
    - **ReplicaciÃ³n en memoria y disco**.
    - **ReejecuciÃ³n de tareas fallidas**.

19. **Â¿QuÃ© es la ejecuciÃ³n especulativa en Spark y cuÃ¡ndo se usa?**  
    Es una funciÃ³n que detecta tareas lentas y las reejecuta en otros nodos para evitar cuellos de botella.

20. **Â¿CÃ³mo se optimiza el nÃºmero de particiones en un trabajo de Spark?**  
    - Ajustando `repartition(n)` o `coalesce(n)` segÃºn la carga de datos.
    - Configurando `spark.default.parallelism` en base al nÃºmero de nÃºcleos disponibles.

### ğŸ”„ Transformaciones y Acciones

21. **Â¿CuÃ¡l es la diferencia entre transformaciones y acciones en Spark?**  
    - **Transformaciones**: Devuelven un nuevo RDD/DataFrame sin ejecutarse inmediatamente. Ejemplo: `map()`, `filter()`, `groupByKey()`.
    - **Acciones**: Ejecutan las transformaciones y devuelven un resultado al driver. Ejemplo: `count()`, `collect()`, `take()`.

22. **Â¿QuÃ© es un narrow transformation y un wide transformation?**  
    - **Narrow Transformation**: Las operaciones afectan solo una particiÃ³n, sin requerir redistribuciÃ³n de datos. Ejemplo: `map()`, `filter()`.
    - **Wide Transformation**: Requiere redistribuir datos entre mÃºltiples particiones (shuffle). Ejemplo: `groupByKey()`, `reduceByKey()`.

23. **Â¿CuÃ¡les son ejemplos de transformaciones comunes en Spark?**  
    - `map()`, `flatMap()`, `filter()`, `groupByKey()`, `reduceByKey()`, `repartition()`, `coalesce()`.

24. **Â¿CuÃ¡les son ejemplos de acciones comunes en Spark?**  
    - `count()`, `collect()`, `take()`, `first()`, `saveAsTextFile()`, `foreach()`.

25. **Â¿QuÃ© es el proceso de Shuffle en Spark?**  
    Es el proceso de redistribuciÃ³n de datos entre particiones, ocurre en operaciones como `groupByKey()` y `join()`, y puede afectar el rendimiento debido al trÃ¡fico de red.

26. **Â¿CÃ³mo se minimiza el Shuffle en Spark?**  
    - Usar `reduceByKey()` en lugar de `groupByKey()`.
    - Ajustar el nÃºmero de particiones con `coalesce()` en lugar de `repartition()`.
    - Evitar joins innecesarios y usar `broadcast()` cuando sea posible.

27. **Â¿CÃ³mo funcionan las transformaciones `map()` y `flatMap()`?**  
    - `map()`: Aplica una funciÃ³n a cada elemento del RDD/DataFrame y devuelve un solo valor por entrada.
    - `flatMap()`: Similar a `map()`, pero permite devolver mÃºltiples valores por entrada, resultando en una estructura aplanada.

28. **Â¿CÃ³mo se diferencian `groupByKey()` y `reduceByKey()`?**  
    - `groupByKey()`: Agrupa los valores por clave sin reducirlos, lo que puede generar un alto uso de memoria y shuffle.
    - `reduceByKey()`: Aplica una funciÃ³n de reducciÃ³n directamente en cada clave antes del shuffle, mejorando el rendimiento.

29. **Â¿Por quÃ© `reduceByKey()` es mÃ¡s eficiente que `groupByKey()`?**  
    `reduceByKey()` realiza la agregaciÃ³n localmente en cada particiÃ³n antes del shuffle, lo que minimiza la cantidad de datos transferidos a travÃ©s de la red.

30. **Â¿CÃ³mo se pueden unir diferentes conjuntos de datos en Spark?**  
    - Usando `join()` para combinar DataFrames/RDDs basados en una clave comÃºn.
    - Optimizando joins con `broadcast()` cuando una de las tablas es pequeÃ±a.

### ğŸ“Š Spark SQL y DataFrames

31. **Â¿QuÃ© es Spark SQL y para quÃ© se utiliza?**  
    Es un mÃ³dulo de Spark que permite consultar datos estructurados usando SQL y APIs de DataFrames/Datasets, proporcionando optimizaciones automÃ¡ticas a travÃ©s del **Catalyst Optimizer**.

32. **Â¿CuÃ¡l es la diferencia entre un DataFrame y un Dataset en Spark?**  
    - **DataFrame**: ColecciÃ³n de datos estructurados similar a una tabla SQL, sin tipado estricto.
    - **Dataset**: Similar a un DataFrame, pero con tipado fuerte y mÃ¡s seguridad en tiempo de compilaciÃ³n (solo disponible en Scala y Java).

33. **Â¿CÃ³mo se crean DataFrames en Spark?**  
    - Desde archivos CSV, JSON, Parquet: `spark.read.format("csv").load("archivo.csv")`.
    - A partir de una consulta SQL: `spark.sql("SELECT * FROM tabla")`.
    - Desde RDDs con `spark.createDataFrame(rdd, schema)`.

34. **Â¿CÃ³mo se puede ejecutar SQL en Spark?**  
    - Usando `spark.sql("SELECT * FROM tabla")`.
    - Creando una vista temporal con `createOrReplaceTempView()` y ejecutando consultas SQL sobre ella.

35. **Â¿QuÃ© es una vista temporal en Spark SQL?**  
    Es una vista lÃ³gica que permite consultar un DataFrame con SQL dentro de una sesiÃ³n de Spark, sin persistencia en disco.

36. **Â¿QuÃ© diferencia hay entre `createOrReplaceTempView()` y `createGlobalTempView()`?**  
    - `createOrReplaceTempView()`: Solo estÃ¡ disponible dentro de la sesiÃ³n de Spark actual.
    - `createGlobalTempView()`: Disponible en todas las sesiones de Spark dentro del clÃºster.

37. **Â¿CÃ³mo se pueden leer datos de diferentes formatos en Spark SQL?**  
    - CSV: `spark.read.csv("archivo.csv")`.
    - JSON: `spark.read.json("archivo.json")`.
    - Parquet: `spark.read.parquet("archivo.parquet")`.
    - ORC: `spark.read.orc("archivo.orc")`.

38. **Â¿QuÃ© es el Catalyst Optimizer en Spark?**  
    Es el optimizador de consultas de Spark SQL, encargado de transformar y mejorar el plan de ejecuciÃ³n de consultas para maximizar el rendimiento.

39. **Â¿CÃ³mo funciona el Predicate Pushdown en Spark?**  
    Es una tÃ©cnica de optimizaciÃ³n que filtra los datos lo mÃ¡s cerca posible de la fuente, reduciendo la cantidad de datos leÃ­dos y mejorando el rendimiento.

40. **Â¿CÃ³mo se pueden escribir resultados en diferentes formatos en Spark SQL?**  
    - CSV: `df.write.csv("salida.csv")`.
    - JSON: `df.write.json("salida.json")`.
    - Parquet: `df.write.parquet("salida.parquet")`.
    - ORC: `df.write.orc("salida.orc")`.

