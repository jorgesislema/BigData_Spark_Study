## ğŸ”¥ Preguntas Intermedias sobre Apache Spark

### ğŸ“Œ General

1. **Â¿QuÃ© es Apache Spark y para quÃ© se utiliza?**  
   Apache Spark es un motor de procesamiento de datos de cÃ³digo abierto, diseÃ±ado para procesar grandes volÃºmenes de datos en paralelo de manera rÃ¡pida y eficiente. Se utiliza en tareas de anÃ¡lisis de datos, machine learning, procesamiento en tiempo real y Big Data.

2. **Â¿CuÃ¡les son los principales mÃ³dulos de Apache Spark?**  
   - **Spark Core**: Maneja la ejecuciÃ³n y administraciÃ³n de tareas.
   - **Spark SQL**: Permite consultas SQL sobre datos estructurados.
   - **Spark Streaming**: Procesamiento de datos en tiempo real.
   - **MLlib**: LibrerÃ­a de Machine Learning.
   - **GraphX**: Procesamiento de datos en grafos.

3. **Â¿CÃ³mo se diferencia Spark de MapReduce en tÃ©rminos de rendimiento?**  
   Spark es significativamente mÃ¡s rÃ¡pido que MapReduce porque procesa los datos en memoria, mientras que MapReduce escribe los resultados intermedios en disco, lo que genera una gran latencia.

4. **Â¿QuÃ© ventajas ofrece Spark sobre otras tecnologÃ­as de procesamiento de datos?**  
   - Procesamiento en memoria para mayor velocidad.
   - API en mÃºltiples lenguajes (Scala, Python, Java, R).
   - Soporte para procesamiento por lotes y en tiempo real.
   - Compatibilidad con Hadoop y otras herramientas de Big Data.

5. **Â¿CuÃ¡les son las desventajas o limitaciones de Spark?**  
   - Alto consumo de memoria.
   - Puede ser mÃ¡s costoso en la nube debido a la necesidad de hardware mÃ¡s potente.
   - Requiere ajustes para optimizar el rendimiento en grandes clÃºsteres.

6. **Â¿En quÃ© escenarios es mÃ¡s Ãºtil usar Spark en lugar de Hadoop?**  
   - Cuando se requiere procesamiento en tiempo real o anÃ¡lisis en memoria.
   - Para ejecutar cargas de trabajo iterativas como Machine Learning.
   - En entornos donde se necesita flexibilidad con diferentes fuentes de datos.

7. **Â¿QuÃ© es el SparkContext y por quÃ© es importante?**  
   Es el punto de entrada principal para cualquier aplicaciÃ³n de Spark. Administra la configuraciÃ³n de la aplicaciÃ³n y la distribuciÃ³n de tareas en el clÃºster.

8. **Â¿CÃ³mo se inicia una sesiÃ³n de Spark?**  
   Se inicia creando un `SparkSession` en cÃ³digo o ejecutando `spark-shell` en la lÃ­nea de comandos.

9. **Â¿QuÃ© es un SparkSession y cuÃ¡l es su funciÃ³n?**  
   Es el punto de entrada unificado para trabajar con Spark, reemplazando la necesidad de `SparkContext`, `SQLContext` y `HiveContext`.

10. **Â¿CuÃ¡les son los principales entornos donde se puede ejecutar Spark?**  
    - Modo local en una sola mÃ¡quina.
    - ClÃºster Hadoop YARN.
    - Apache Mesos.
    - Kubernetes.
    - Servicios en la nube como AWS EMR, Databricks, Google Cloud Dataproc.

### âš¡ Arquitectura y Funcionamiento

11. **Â¿QuÃ© es un RDD y cÃ³mo se diferencia de un DataFrame?**  
    - **RDD (Resilient Distributed Dataset)**: Estructura fundamental en Spark, basada en datos distribuidos y procesamiento inmutable.
    - **DataFrame**: ColecciÃ³n de datos organizados en formato tabular, optimizados con Catalyst Optimizer y mÃ¡s fÃ¡ciles de usar que los RDDs.

12. **Â¿CÃ³mo se crean y transforman los RDDs en Spark?**  
    - Desde colecciones locales usando `parallelize()`.
    - Desde archivos almacenados en HDFS, S3, etc.
    - Aplicando transformaciones como `map()`, `filter()`, `flatMap()`.

13. **Â¿QuÃ© operaciones se pueden realizar en un RDD?**  
    - **Transformaciones**: `map()`, `filter()`, `reduceByKey()`.
    - **Acciones**: `count()`, `collect()`, `take()`.

14. **Â¿QuÃ© es el Lazy Evaluation en Spark y por quÃ© es importante?**  
    Significa que las transformaciones en Spark no se ejecutan inmediatamente, sino que se construye un plan de ejecuciÃ³n que solo se activa cuando se realiza una acciÃ³n. Esto optimiza el rendimiento y reduce la sobrecarga de cÃ¡lculo.

15. **Â¿QuÃ© es el Directed Acyclic Graph (DAG) en Spark?**  
    Es un grÃ¡fico que representa la secuencia de operaciones en una tarea de Spark, optimizando la ejecuciÃ³n y eliminando redundancias.

16. **Â¿CÃ³mo funciona la planificaciÃ³n de tareas en Spark?**  
    - El Driver convierte las operaciones en un DAG.
    - Divide el DAG en Stages.
    - Distribuye tareas entre ejecutores en un clÃºster.

17. **Â¿QuÃ© papel juegan los ejecutores en la ejecuciÃ³n de tareas?**  
    Son procesos que ejecutan tareas asignadas por el Driver y administran la memoria para el almacenamiento en cachÃ©.

18. **Â¿CÃ³mo maneja Spark la tolerancia a fallos?**  
    - **Lineage de RDDs**: Permite reconstruir datos a partir de transformaciones previas.
    - **ReplicaciÃ³n en memoria y disco**.
    - **ReejecuciÃ³n de tareas fallidas**.

19. **Â¿QuÃ© es la ejecuciÃ³n especulativa en Spark y cuÃ¡ndo se usa?**  
    Es una funciÃ³n que detecta tareas lentas y las reejecuta en otros nodos para evitar cuellos de botella.

20. **Â¿CÃ³mo se optimiza el nÃºmero de particiones en un trabajo de Spark?**  
    - Ajustando `repartition(n)` o `coalesce(n)` segÃºn la carga de datos.
    - Configurando `spark.default.parallelism` en base al nÃºmero de nÃºcleos disponibles.

