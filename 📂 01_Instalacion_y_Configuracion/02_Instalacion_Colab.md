# ğŸš€ InstalaciÃ³n de Apache Spark en Google Colab

## ğŸ”¥ IntroducciÃ³n
Google Colab es una plataforma en la nube que permite ejecutar cÃ³digo en Python sin necesidad de configuraciones locales. Sin embargo, no incluye Apache Spark por defecto, por lo que es necesario instalarlo manualmente.

---

## ğŸ“Œ Requisitos Previos
- Cuenta en Google ([https://colab.research.google.com](https://colab.research.google.com)).
- Conocimientos bÃ¡sicos de Python y Jupyter Notebooks.

---

## ğŸ› ï¸ InstalaciÃ³n de Apache Spark en Colab
Como Google Colab no incluye Apache Spark, debemos instalarlo desde cero usando los siguientes pasos.

### ğŸ”¹ Paso 1: Descargar e Instalar Apache Spark
Ejecuta el siguiente cÃ³digo en una celda de Colab para instalar Spark y sus dependencias:
```python
!apt-get install openjdk-8-jdk-headless -qq > /dev/null
!wget -q https://downloads.apache.org/spark/spark-3.2.1/spark-3.2.1-bin-hadoop3.2.tgz
!tar xf spark-3.2.1-bin-hadoop3.2.tgz
!pip install -q findspark
```

### ğŸ”¹ Paso 2: Configurar Variables de Entorno
Configura las variables de entorno necesarias para ejecutar Spark:
```python
import os
os.environ["JAVA_HOME"] = "/usr/lib/jvm/java-8-openjdk-amd64"
os.environ["SPARK_HOME"] = "/content/spark-3.2.1-bin-hadoop3.2"
```

### ğŸ”¹ Paso 3: Iniciar PySpark en Colab
Ejecuta el siguiente cÃ³digo para iniciar PySpark:
```python
import findspark
findspark.init()
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("Colab_Spark").getOrCreate()
print("Apache Spark en Colab ha sido configurado correctamente.")
```
Si el proceso es exitoso, verÃ¡s el mensaje de confirmaciÃ³n.

---

## ğŸï¸ VerificaciÃ³n de InstalaciÃ³n
Para asegurarte de que Spark estÃ¡ funcionando, ejecuta:
```python
df = spark.createDataFrame([("Alice", 25), ("Bob", 30), ("Charlie", 35)], ["Nombre", "Edad"])
df.show()
```
Si ves una tabla con nombres y edades, significa que Spark estÃ¡ corriendo correctamente en Colab.

---

## ğŸ¯ ConclusiÃ³n
Ahora tienes Apache Spark funcionando en **Google Colab**. Esto te permite aprovechar la infraestructura en la nube sin necesidad de configuraciones complejas. Puedes comenzar a trabajar con **PySpark** para anÃ¡lisis de datos y machine learning en grandes volÃºmenes de informaciÃ³n. ğŸš€

