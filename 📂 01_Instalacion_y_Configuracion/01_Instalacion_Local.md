# ğŸ–¥ï¸ InstalaciÃ³n de Apache Spark en Local

## ğŸ”¥ IntroducciÃ³n
Apache Spark se puede ejecutar en entornos locales sin necesidad de un clÃºster distribuido. Esta instalaciÃ³n es ideal para desarrollo, pruebas y aprendizaje antes de trabajar en entornos de producciÃ³n.

---

## ğŸ› ï¸ Requisitos Previos
Antes de instalar Spark, asegÃºrate de tener lo siguiente:
- **Java JDK 8 o superior**: Spark requiere Java para ejecutarse.
- **Python 3.6 o superior**: Para usar PySpark.
- **Apache Spark**: Descarga e instala la versiÃ³n estable.
- **Hadoop (opcional)**: Para utilizar HDFS y gestionar almacenamiento distribuido.

### ğŸ“Œ Verificar Instalaciones
Ejecuta los siguientes comandos en la terminal para comprobar las versiones instaladas:
```bash
java -version
python --version
```
Si no estÃ¡n instalados, descÃ¡rgalos desde:
- Java: [https://www.oracle.com/java/](https://www.oracle.com/java/)
- Python: [https://www.python.org/](https://www.python.org/)

---

## ğŸ“¥ InstalaciÃ³n de Apache Spark

### ğŸ”¹ Paso 1: Descargar Apache Spark
Descarga la Ãºltima versiÃ³n estable desde el sitio oficial:  
â¡ï¸ [https://spark.apache.org/downloads.html](https://spark.apache.org/downloads.html)  
Selecciona **Pre-built for Apache Hadoop** para evitar compilaciones manuales.

### ğŸ”¹ Paso 2: Extraer y Configurar Variables de Entorno
Una vez descargado el archivo, extrÃ¡elo en el directorio deseado y configura las variables de entorno:
```bash
mkdir ~/spark
cd ~/spark
tar -xvzf ~/Descargas/spark-3.x.x-bin-hadoop3.tgz
```
Edita el archivo `.bashrc` o `.zshrc` para agregar las variables de entorno:
```bash
echo 'export SPARK_HOME=~/spark/spark-3.x.x-bin-hadoop3' >> ~/.bashrc
echo 'export PATH=$SPARK_HOME/bin:$PATH' >> ~/.bashrc
echo 'export PYSPARK_PYTHON=python3' >> ~/.bashrc
source ~/.bashrc
```

### ğŸ”¹ Paso 3: Verificar la InstalaciÃ³n
Para comprobar que Spark estÃ¡ funcionando correctamente, ejecuta:
```bash
spark-shell
```
Si todo estÃ¡ correcto, verÃ¡s la consola interactiva de Spark con Scala.

---

## ğŸ Uso de PySpark en Local
Si deseas ejecutar Spark con Python, usa:
```bash
pyspark
```
Para probar un pequeÃ±o script en PySpark:
```python
from pyspark.sql import SparkSession

spark = SparkSession.builder.appName("EjemploLocal").getOrCreate()
data = [("Alice", 25), ("Bob", 30), ("Charlie", 35)]
df = spark.createDataFrame(data, ["Nombre", "Edad"])
df.show()
```

---

## ğŸš€ ConclusiÃ³n
Con esta configuraciÃ³n, tienes Apache Spark funcionando en tu mÃ¡quina local. Puedes empezar a desarrollar y probar cÃ³digo en PySpark sin necesidad de un clÃºster distribuido. Â¡A explorar el poder de Spark! ğŸ”¥

