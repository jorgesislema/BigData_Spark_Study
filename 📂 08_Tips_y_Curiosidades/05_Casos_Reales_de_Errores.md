# üö® Casos Reales de Errores en Apache Spark y C√≥mo Solucionarlos

## üî• Introducci√≥n
Apache Spark es una herramienta poderosa, pero su ejecuci√≥n distribuida puede generar **errores dif√≠ciles de depurar**. Aqu√≠ analizamos **casos reales de errores**, su causa y la mejor forma de solucionarlos.

---

## ‚ö†Ô∏è 1Ô∏è‚É£ Error: `OutOfMemoryError`
### ‚ùå Problema:
El trabajo de Spark consume m√°s memoria de la disponible en el ejecutor.

### ‚úÖ Soluci√≥n:
‚úî **Ajustar la memoria de los ejecutores**:
```python
spark.conf.set("spark.executor.memory", "8g")
```
‚úî **Evitar `collect()` en grandes vol√∫menes de datos**:
```python
df.take(10)  # En lugar de df.collect()
```
‚úî **Usar `cache()` con precauci√≥n**:
```python
df.persist()
```

---

## ‚ö†Ô∏è 2Ô∏è‚É£ Error: `Task Not SerializableException`
### ‚ùå Problema:
Spark no puede serializar una funci√≥n o variable dentro de una transformaci√≥n distribuida.

### ‚úÖ Soluci√≥n:
‚úî **Evitar el uso de objetos complejos en `map()`**:
```python
def transform(row):
    return row["columna"] * 2

rdd.map(transform)  # ‚ö†Ô∏è Esto puede fallar
```
‚úî **Usar funciones lambda o `udf()` de PySpark**:
```python
from pyspark.sql.functions import udf
from pyspark.sql.types import IntegerType

def multiply_by_two(x):
    return x * 2

multiply_udf = udf(multiply_by_two, IntegerType())
df = df.withColumn("new_col", multiply_udf(df["columna"]))
```

---

## ‚ö†Ô∏è 3Ô∏è‚É£ Error: `Job Aborted due to Stage Failure`
### ‚ùå Problema:
Un **shuffle** excesivo o fallos en la red pueden abortar el trabajo de Spark.

### ‚úÖ Soluci√≥n:
‚úî **Reducir el n√∫mero de particiones en Shuffle**:
```python
spark.conf.set("spark.sql.shuffle.partitions", "200")
```
‚úî **Optimizar `joins` con `broadcast()`**:
```python
from pyspark.sql.functions import broadcast

df1.join(broadcast(df2), "id").show()
```
‚úî **Verificar errores en los ejecutores con**:
```python
spark.sparkContext.statusTracker.getExecutorInfos()
```

---

## ‚ö†Ô∏è 4Ô∏è‚É£ Error: `FileNotFoundException` en HDFS o S3
### ‚ùå Problema:
Spark no encuentra un archivo en HDFS o S3.

### ‚úÖ Soluci√≥n:
‚úî **Verificar la ruta del archivo**:
```python
hdfs dfs -ls /ruta/archivo.parquet
```
‚úî **Usar rutas completas y con prefijos correctos**:
```python
df = spark.read.parquet("s3a://mi-bucket/datos.parquet")
```
‚úî **Revisar permisos de acceso en S3 o HDFS**:
```python
spark.conf.set("fs.s3a.access.key", "TU_ACCESS_KEY")
```

---

## üéØ Conclusi√≥n
Detectar y corregir errores en Apache Spark requiere **conocimiento de logs, optimizaci√≥n y configuraci√≥n**. Aplicar buenas pr√°cticas ayuda a prevenir fallos y mejorar el rendimiento. üöÄ

